{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# ğŸš€ AI ML Pipeline - Colab GPU è®­ç»ƒç¯å¢ƒ\n",
        "\n",
        "è¿™ä¸ªç¬”è®°æœ¬å¸®åŠ©æ‚¨å°†æœ¬åœ°ä»£ç åŒæ­¥åˆ°Colabï¼Œå¹¶ä½¿ç”¨GPUè¿›è¡Œè®­ç»ƒã€‚\n",
        "\n",
        "## ä½¿ç”¨æ­¥éª¤\n",
        "1. è¿è¡Œç¯å¢ƒè®¾ç½®\n",
        "2. åŒæ­¥GitHubä»£ç \n",
        "3. å®‰è£…ä¾èµ–\n",
        "4. å¼€å§‹è®­ç»ƒ\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 1. ç¯å¢ƒè®¾ç½®å’Œæ£€æŸ¥\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# æ£€æŸ¥GPUå¯ç”¨æ€§\n",
        "import torch\n",
        "import os\n",
        "import sys\n",
        "from google.colab import drive\n",
        "\n",
        "print(f\"PyTorchç‰ˆæœ¬: {torch.__version__}\")\n",
        "print(f\"CUDAå¯ç”¨: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPUè®¾å¤‡: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
        "else:\n",
        "    print(\"âš ï¸ æ²¡æœ‰æ£€æµ‹åˆ°GPUï¼è¯·æ£€æŸ¥è¿è¡Œæ—¶è®¾ç½®\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# æŒ‚è½½Google Driveå¹¶åˆ›å»ºå·¥ä½œç›®å½•\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "work_dir = '/content/ai-ml-pipeline'\n",
        "os.makedirs(work_dir, exist_ok=True)\n",
        "os.chdir(work_dir)\n",
        "print(f\"å·¥ä½œç›®å½•: {os.getcwd()}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2. å¿«é€Ÿè®¾ç½®ï¼ˆæ¨èï¼‰\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ä¸€é”®è®¾ç½®æ•´ä¸ªç¯å¢ƒ\n",
        "# è¯·å°†ä¸‹é¢çš„GitHubä»“åº“åœ°å€æ›¿æ¢ä¸ºæ‚¨çš„ä»“åº“\n",
        "GITHUB_REPO = \"\"  # ä¾‹å¦‚: \"https://github.com/username/ai-ml-pipeline.git\"\n",
        "\n",
        "if GITHUB_REPO:\n",
        "    import subprocess\n",
        "    result = subprocess.run(['git', 'clone', GITHUB_REPO, '.'], capture_output=True, text=True)\n",
        "    if result.returncode == 0:\n",
        "        print(\"âœ… ä»£ç å·²ä»GitHubåŒæ­¥\")\n",
        "    else:\n",
        "        print(f\"âš ï¸ å…‹éš†å¤±è´¥: {result.stderr}\")\n",
        "else:\n",
        "    print(\"ğŸ’¡ è¯·è®¾ç½®GITHUB_REPOå˜é‡æˆ–ä½¿ç”¨æ‰‹åŠ¨ä¸Šä¼ æ–¹å¼\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# å®‰è£…ä¾èµ–å’Œè®¾ç½®ç¯å¢ƒ\n",
        "import subprocess\n",
        "packages = ['torch', 'torchvision', 'torchaudio', 'transformers', 'datasets', 'wandb', 'tqdm', 'matplotlib', 'seaborn', 'scikit-learn']\n",
        "\n",
        "for package in packages:\n",
        "    subprocess.run(['pip', 'install', '-q', package], capture_output=True)\n",
        "\n",
        "# æ·»åŠ é¡¹ç›®è·¯å¾„åˆ°Pythonè·¯å¾„\n",
        "import sys\n",
        "sys.path.append('/content/ai-ml-pipeline/src')\n",
        "sys.path.append('/content/ai-ml-pipeline/utils')\n",
        "\n",
        "print(\"âœ… ç¯å¢ƒè®¾ç½®å®Œæˆï¼\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 3. å¼€å§‹è®­ç»ƒ\n",
        "\n",
        "ç°åœ¨æ‚¨å¯ä»¥å¼€å§‹ä½¿ç”¨GPUè¿›è¡Œè®­ç»ƒäº†ï¼\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ç¤ºä¾‹ï¼šå¯åŠ¨è®­ç»ƒ\n",
        "print(\"ğŸš€ å‡†å¤‡å¼€å§‹è®­ç»ƒ...\")\n",
        "\n",
        "# æ£€æŸ¥é¡¹ç›®ç»“æ„\n",
        "import os\n",
        "print(\"\\nğŸ“ é¡¹ç›®æ–‡ä»¶:\")\n",
        "for root, dirs, files in os.walk('.'):\n",
        "    dirs[:] = [d for d in dirs if not d.startswith('.') and d != '__pycache__']\n",
        "    level = root.replace('.', '').count(os.sep)\n",
        "    if level < 3:  # é™åˆ¶æ˜¾ç¤ºæ·±åº¦\n",
        "        indent = ' ' * 2 * level\n",
        "        print(f\"{indent}{os.path.basename(root)}/\")\n",
        "\n",
        "# å¦‚æœæœ‰è®­ç»ƒè„šæœ¬ï¼Œå¯ä»¥è¿™æ ·è¿è¡Œï¼š\n",
        "# from src.training.train import main\n",
        "# main()\n",
        "\n",
        "print(\"\\nğŸ’¡ æç¤º: åœ¨src/ç›®å½•ä¸‹ç¼–å†™æ‚¨çš„è®­ç»ƒä»£ç \")\n",
        "print(\"ğŸ’¡ æç¤º: ä½¿ç”¨configs/ç›®å½•ç®¡ç†è®­ç»ƒé…ç½®\")\n",
        "print(\"ğŸ’¡ æç¤º: è®­ç»ƒå®Œæˆåè®°å¾—ä¿å­˜æ¨¡å‹åˆ°Google Drive\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
