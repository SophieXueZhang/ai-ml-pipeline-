#!/usr/bin/env python3
"""
æ–‡ä»¶æ¸…ç†å·¥å…·
åŠŸèƒ½ï¼š
1. æ£€æµ‹é‡å¤æ–‡ä»¶
2. æ£€æµ‹é•¿æ—¶é—´æœªä½¿ç”¨çš„æ–‡ä»¶
3. æä¾›å®‰å…¨çš„æ¸…ç†é€‰é¡¹
"""

import os
import hashlib
import time
from datetime import datetime, timedelta
from pathlib import Path
import argparse
from collections import defaultdict
import json
import shutil

class FileCleanerTool:
    def __init__(self, target_dir="."):
        self.target_dir = Path(target_dir).resolve()
        self.duplicate_files = defaultdict(list)
        self.old_files = []
        
    def calculate_file_hash(self, file_path):
        """è®¡ç®—æ–‡ä»¶çš„MD5å“ˆå¸Œå€¼"""
        try:
            hash_md5 = hashlib.md5()
            with open(file_path, "rb") as f:
                # åˆ†å—è¯»å–ï¼Œé¿å…å¤§æ–‡ä»¶å ç”¨è¿‡å¤šå†…å­˜
                for chunk in iter(lambda: f.read(4096), b""):
                    hash_md5.update(chunk)
            return hash_md5.hexdigest()
        except Exception as e:
            print(f"è®¡ç®—å“ˆå¸Œå€¼å¤±è´¥ {file_path}: {e}")
            return None
    
    def find_duplicate_files(self, file_extensions=None, min_size=1024):
        """
        æŸ¥æ‰¾é‡å¤æ–‡ä»¶
        file_extensions: è¦æ£€æŸ¥çš„æ–‡ä»¶æ‰©å±•ååˆ—è¡¨ï¼ŒNoneè¡¨ç¤ºæ£€æŸ¥æ‰€æœ‰æ–‡ä»¶
        min_size: æœ€å°æ–‡ä»¶å¤§å°ï¼ˆå­—èŠ‚ï¼‰ï¼Œé¿å…æ£€æŸ¥ç©ºæ–‡ä»¶æˆ–å¾ˆå°çš„æ–‡ä»¶
        """
        print(f"æ­£åœ¨æ‰«æç›®å½•: {self.target_dir}")
        print(f"æŸ¥æ‰¾é‡å¤æ–‡ä»¶ï¼ˆæœ€å°å¤§å°: {min_size} å­—èŠ‚ï¼‰...")
        
        file_hashes = defaultdict(list)
        
        for root, dirs, files in os.walk(self.target_dir):
            # è·³è¿‡éšè—ç›®å½•
            dirs[:] = [d for d in dirs if not d.startswith('.')]
            
            for file in files:
                # è·³è¿‡éšè—æ–‡ä»¶
                if file.startswith('.'):
                    continue
                    
                file_path = Path(root) / file
                
                # æ£€æŸ¥æ–‡ä»¶æ‰©å±•å
                if file_extensions and file_path.suffix.lower() not in file_extensions:
                    continue
                
                try:
                    # æ£€æŸ¥æ–‡ä»¶å¤§å°
                    if file_path.stat().st_size < min_size:
                        continue
                    
                    # è®¡ç®—å“ˆå¸Œå€¼
                    file_hash = self.calculate_file_hash(file_path)
                    if file_hash:
                        file_info = {
                            'path': str(file_path),
                            'size': file_path.stat().st_size,
                            'modified': datetime.fromtimestamp(file_path.stat().st_mtime)
                        }
                        file_hashes[file_hash].append(file_info)
                        
                except Exception as e:
                    print(f"å¤„ç†æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
        
        # æ‰¾å‡ºé‡å¤æ–‡ä»¶
        self.duplicate_files = {hash_val: files for hash_val, files in file_hashes.items() if len(files) > 1}
        
        return self.duplicate_files
    
    def find_old_files(self, days=30, file_extensions=None):
        """
        æŸ¥æ‰¾é•¿æ—¶é—´æœªä½¿ç”¨çš„æ–‡ä»¶
        days: å¤šå°‘å¤©æœªä¿®æ”¹ç®—ä½œæ—§æ–‡ä»¶
        file_extensions: è¦æ£€æŸ¥çš„æ–‡ä»¶æ‰©å±•ååˆ—è¡¨
        """
        print(f"æŸ¥æ‰¾ {days} å¤©å†…æœªä¿®æ”¹çš„æ–‡ä»¶...")
        
        cutoff_date = datetime.now() - timedelta(days=days)
        self.old_files = []
        
        for root, dirs, files in os.walk(self.target_dir):
            # è·³è¿‡éšè—ç›®å½•
            dirs[:] = [d for d in dirs if not d.startswith('.')]
            
            for file in files:
                # è·³è¿‡éšè—æ–‡ä»¶
                if file.startswith('.'):
                    continue
                    
                file_path = Path(root) / file
                
                # æ£€æŸ¥æ–‡ä»¶æ‰©å±•å
                if file_extensions and file_path.suffix.lower() not in file_extensions:
                    continue
                
                try:
                    stat = file_path.stat()
                    # ä½¿ç”¨æœ€åä¿®æ”¹æ—¶é—´å’Œæœ€åè®¿é—®æ—¶é—´ä¸­çš„è¾ƒæ–°è€…
                    last_used = max(
                        datetime.fromtimestamp(stat.st_mtime),
                        datetime.fromtimestamp(stat.st_atime)
                    )
                    
                    if last_used < cutoff_date:
                        file_info = {
                            'path': str(file_path),
                            'size': stat.st_size,
                            'last_modified': datetime.fromtimestamp(stat.st_mtime),
                            'last_accessed': datetime.fromtimestamp(stat.st_atime),
                            'last_used': last_used
                        }
                        self.old_files.append(file_info)
                        
                except Exception as e:
                    print(f"å¤„ç†æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
        
        # æŒ‰æœ€åä½¿ç”¨æ—¶é—´æ’åº
        self.old_files.sort(key=lambda x: x['last_used'])
        return self.old_files
    
    def format_size(self, size_bytes):
        """æ ¼å¼åŒ–æ–‡ä»¶å¤§å°"""
        for unit in ['B', 'KB', 'MB', 'GB']:
            if size_bytes < 1024.0:
                return f"{size_bytes:.1f} {unit}"
            size_bytes /= 1024.0
        return f"{size_bytes:.1f} TB"
    
    def print_duplicate_report(self):
        """æ‰“å°é‡å¤æ–‡ä»¶æŠ¥å‘Š"""
        if not self.duplicate_files:
            print("âœ… æ²¡æœ‰æ‰¾åˆ°é‡å¤æ–‡ä»¶")
            return
        
        total_wasted_space = 0
        print(f"\nğŸ“‹ æ‰¾åˆ° {len(self.duplicate_files)} ç»„é‡å¤æ–‡ä»¶ï¼š")
        print("=" * 80)
        
        for i, (hash_val, files) in enumerate(self.duplicate_files.items(), 1):
            print(f"\nç»„ {i} (MD5: {hash_val[:8]}...):")
            print(f"  æ–‡ä»¶å¤§å°: {self.format_size(files[0]['size'])}")
            print(f"  é‡å¤æ•°é‡: {len(files)} ä¸ªæ–‡ä»¶")
            
            # è®¡ç®—æµªè´¹çš„ç©ºé—´ï¼ˆä¿ç•™ä¸€ä¸ªåŸæ–‡ä»¶ï¼‰
            wasted_space = files[0]['size'] * (len(files) - 1)
            total_wasted_space += wasted_space
            print(f"  æµªè´¹ç©ºé—´: {self.format_size(wasted_space)}")
            
            for j, file_info in enumerate(files):
                marker = "ğŸ“ [ä¿ç•™]" if j == 0 else "ğŸ—‘ï¸ [å¯åˆ é™¤]"
                print(f"    {marker} {file_info['path']}")
                print(f"        ä¿®æ”¹æ—¶é—´: {file_info['modified'].strftime('%Y-%m-%d %H:%M:%S')}")
        
        print(f"\nğŸ’¾ æ€»è®¡å¯èŠ‚çœç©ºé—´: {self.format_size(total_wasted_space)}")
    
    def print_old_files_report(self):
        """æ‰“å°æ—§æ–‡ä»¶æŠ¥å‘Š"""
        if not self.old_files:
            print("âœ… æ²¡æœ‰æ‰¾åˆ°é•¿æ—¶é—´æœªä½¿ç”¨çš„æ–‡ä»¶")
            return
        
        total_size = sum(file['size'] for file in self.old_files)
        print(f"\nğŸ“‹ æ‰¾åˆ° {len(self.old_files)} ä¸ªé•¿æ—¶é—´æœªä½¿ç”¨çš„æ–‡ä»¶ï¼š")
        print(f"ğŸ’¾ æ€»å¤§å°: {self.format_size(total_size)}")
        print("=" * 80)
        
        for file_info in self.old_files:
            print(f"\nğŸ“„ {file_info['path']}")
            print(f"   å¤§å°: {self.format_size(file_info['size'])}")
            print(f"   æœ€åä¿®æ”¹: {file_info['last_modified'].strftime('%Y-%m-%d %H:%M:%S')}")
            print(f"   æœ€åè®¿é—®: {file_info['last_accessed'].strftime('%Y-%m-%d %H:%M:%S')}")
            days_ago = (datetime.now() - file_info['last_used']).days
            print(f"   {days_ago} å¤©æœªä½¿ç”¨")
    
    def clean_duplicate_files(self, auto_clean=False):
        """
        æ¸…ç†é‡å¤æ–‡ä»¶
        auto_clean: æ˜¯å¦è‡ªåŠ¨æ¸…ç†ï¼ˆä¿ç•™æœ€æ–°çš„æ–‡ä»¶ï¼‰
        """
        if not self.duplicate_files:
            print("æ²¡æœ‰é‡å¤æ–‡ä»¶éœ€è¦æ¸…ç†")
            return
        
        cleaned_count = 0
        saved_space = 0
        
        for hash_val, files in self.duplicate_files.items():
            # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œä¿ç•™æœ€æ–°çš„æ–‡ä»¶
            files.sort(key=lambda x: x['modified'], reverse=True)
            files_to_delete = files[1:]  # åˆ é™¤é™¤äº†æœ€æ–°æ–‡ä»¶ä¹‹å¤–çš„æ‰€æœ‰æ–‡ä»¶
            
            print(f"\nå¤„ç†é‡å¤æ–‡ä»¶ç»„ (ä¿ç•™: {files[0]['path']}):")
            
            for file_info in files_to_delete:
                file_path = Path(file_info['path'])
                if not file_path.exists():
                    continue
                
                if auto_clean:
                    try:
                        file_path.unlink()
                        print(f"  âœ… å·²åˆ é™¤: {file_path}")
                        cleaned_count += 1
                        saved_space += file_info['size']
                    except Exception as e:
                        print(f"  âŒ åˆ é™¤å¤±è´¥: {file_path} - {e}")
                else:
                    response = input(f"  åˆ é™¤ {file_path}? [y/N/q]: ").strip().lower()
                    if response == 'q':
                        print("é€€å‡ºæ¸…ç†è¿‡ç¨‹")
                        break
                    elif response == 'y':
                        try:
                            file_path.unlink()
                            print(f"  âœ… å·²åˆ é™¤: {file_path}")
                            cleaned_count += 1
                            saved_space += file_info['size']
                        except Exception as e:
                            print(f"  âŒ åˆ é™¤å¤±è´¥: {file_path} - {e}")
                    else:
                        print(f"  â­ï¸ è·³è¿‡: {file_path}")
        
        print(f"\nğŸ‰ æ¸…ç†å®Œæˆ!")
        print(f"ğŸ“Š åˆ é™¤äº† {cleaned_count} ä¸ªé‡å¤æ–‡ä»¶")
        print(f"ğŸ’¾ èŠ‚çœç©ºé—´: {self.format_size(saved_space)}")
    
    def clean_old_files(self, auto_clean=False):
        """
        æ¸…ç†æ—§æ–‡ä»¶
        auto_clean: æ˜¯å¦è‡ªåŠ¨æ¸…ç†
        """
        if not self.old_files:
            print("æ²¡æœ‰æ—§æ–‡ä»¶éœ€è¦æ¸…ç†")
            return
        
        cleaned_count = 0
        saved_space = 0
        
        for file_info in self.old_files:
            file_path = Path(file_info['path'])
            if not file_path.exists():
                continue
            
            days_ago = (datetime.now() - file_info['last_used']).days
            
            if auto_clean:
                try:
                    file_path.unlink()
                    print(f"âœ… å·²åˆ é™¤: {file_path} ({days_ago}å¤©æœªä½¿ç”¨)")
                    cleaned_count += 1
                    saved_space += file_info['size']
                except Exception as e:
                    print(f"âŒ åˆ é™¤å¤±è´¥: {file_path} - {e}")
            else:
                print(f"\nğŸ“„ {file_path}")
                print(f"   å¤§å°: {self.format_size(file_info['size'])}, {days_ago}å¤©æœªä½¿ç”¨")
                response = input("  åˆ é™¤æ­¤æ–‡ä»¶? [y/N/q]: ").strip().lower()
                if response == 'q':
                    print("é€€å‡ºæ¸…ç†è¿‡ç¨‹")
                    break
                elif response == 'y':
                    try:
                        file_path.unlink()
                        print(f"  âœ… å·²åˆ é™¤: {file_path}")
                        cleaned_count += 1
                        saved_space += file_info['size']
                    except Exception as e:
                        print(f"  âŒ åˆ é™¤å¤±è´¥: {file_path} - {e}")
                else:
                    print(f"  â­ï¸ è·³è¿‡: {file_path}")
        
        print(f"\nğŸ‰ æ¸…ç†å®Œæˆ!")
        print(f"ğŸ“Š åˆ é™¤äº† {cleaned_count} ä¸ªæ—§æ–‡ä»¶")
        print(f"ğŸ’¾ èŠ‚çœç©ºé—´: {self.format_size(saved_space)}")
    
    def save_report(self, filename="file_cleanup_report.json"):
        """ä¿å­˜æ¸…ç†æŠ¥å‘Šåˆ°JSONæ–‡ä»¶"""
        report = {
            'scan_time': datetime.now().isoformat(),
            'target_directory': str(self.target_dir),
            'duplicate_files': {
                'groups': len(self.duplicate_files),
                'total_files': sum(len(files) for files in self.duplicate_files.values()),
                'details': {}
            },
            'old_files': {
                'count': len(self.old_files),
                'total_size': sum(file['size'] for file in self.old_files),
                'details': []
            }
        }
        
        # æ·»åŠ é‡å¤æ–‡ä»¶è¯¦æƒ…
        for i, (hash_val, files) in enumerate(self.duplicate_files.items()):
            report['duplicate_files']['details'][f'group_{i+1}'] = {
                'hash': hash_val,
                'file_size': files[0]['size'],
                'file_count': len(files),
                'files': [{'path': f['path'], 'modified': f['modified'].isoformat()} for f in files]
            }
        
        # æ·»åŠ æ—§æ–‡ä»¶è¯¦æƒ…
        for file_info in self.old_files:
            report['old_files']['details'].append({
                'path': file_info['path'],
                'size': file_info['size'],
                'last_used': file_info['last_used'].isoformat(),
                'days_unused': (datetime.now() - file_info['last_used']).days
            })
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“„ æŠ¥å‘Šå·²ä¿å­˜åˆ°: {filename}")


def main():
    parser = argparse.ArgumentParser(description="æ–‡ä»¶æ¸…ç†å·¥å…·")
    parser.add_argument("directory", nargs="?", default=".", help="è¦æ¸…ç†çš„ç›®å½•è·¯å¾„")
    parser.add_argument("--days", type=int, default=30, help="å¤šå°‘å¤©æœªä½¿ç”¨ç®—ä½œæ—§æ–‡ä»¶ (é»˜è®¤: 30)")
    parser.add_argument("--min-size", type=int, default=1024, help="æ£€æŸ¥é‡å¤æ–‡ä»¶çš„æœ€å°å¤§å° (å­—èŠ‚, é»˜è®¤: 1024)")
    parser.add_argument("--extensions", nargs="+", help="åªæ£€æŸ¥æŒ‡å®šæ‰©å±•åçš„æ–‡ä»¶ (ä¾‹å¦‚: .ipynb .py .txt)")
    parser.add_argument("--auto-clean-duplicates", action="store_true", help="è‡ªåŠ¨æ¸…ç†é‡å¤æ–‡ä»¶")
    parser.add_argument("--auto-clean-old", action="store_true", help="è‡ªåŠ¨æ¸…ç†æ—§æ–‡ä»¶")
    parser.add_argument("--report-only", action="store_true", help="åªç”ŸæˆæŠ¥å‘Šï¼Œä¸æ‰§è¡Œæ¸…ç†")
    parser.add_argument("--save-report", help="ä¿å­˜æŠ¥å‘Šåˆ°æŒ‡å®šæ–‡ä»¶")
    
    args = parser.parse_args()
    
    # åˆ›å»ºæ¸…ç†å·¥å…·å®ä¾‹
    cleaner = FileCleanerTool(args.directory)
    
    print("ğŸ§¹ æ–‡ä»¶æ¸…ç†å·¥å…·å¯åŠ¨")
    print(f"ğŸ“ ç›®æ ‡ç›®å½•: {cleaner.target_dir}")
    print("=" * 80)
    
    # æŸ¥æ‰¾é‡å¤æ–‡ä»¶
    print("ğŸ” æ­¥éª¤ 1: æŸ¥æ‰¾é‡å¤æ–‡ä»¶...")
    duplicate_files = cleaner.find_duplicate_files(
        file_extensions=args.extensions,
        min_size=args.min_size
    )
    cleaner.print_duplicate_report()
    
    # æŸ¥æ‰¾æ—§æ–‡ä»¶
    print(f"\nğŸ” æ­¥éª¤ 2: æŸ¥æ‰¾ {args.days} å¤©å†…æœªä½¿ç”¨çš„æ–‡ä»¶...")
    old_files = cleaner.find_old_files(
        days=args.days,
        file_extensions=args.extensions
    )
    cleaner.print_old_files_report()
    
    # ä¿å­˜æŠ¥å‘Š
    if args.save_report:
        cleaner.save_report(args.save_report)
    
    # å¦‚æœåªæ˜¯ç”ŸæˆæŠ¥å‘Šï¼Œåˆ™é€€å‡º
    if args.report_only:
        print("\nğŸ“‹ ä»…ç”ŸæˆæŠ¥å‘Šæ¨¡å¼ï¼Œä¸æ‰§è¡Œæ¸…ç†æ“ä½œ")
        return
    
    # æ¸…ç†é‡å¤æ–‡ä»¶
    if duplicate_files:
        print("\nğŸ§¹ æ­¥éª¤ 3: æ¸…ç†é‡å¤æ–‡ä»¶...")
        if not args.auto_clean_duplicates:
            response = input("æ˜¯å¦å¼€å§‹æ¸…ç†é‡å¤æ–‡ä»¶? [y/N]: ").strip().lower()
            if response != 'y':
                print("è·³è¿‡é‡å¤æ–‡ä»¶æ¸…ç†")
            else:
                cleaner.clean_duplicate_files(auto_clean=False)
        else:
            cleaner.clean_duplicate_files(auto_clean=True)
    
    # æ¸…ç†æ—§æ–‡ä»¶
    if old_files:
        print("\nğŸ§¹ æ­¥éª¤ 4: æ¸…ç†æ—§æ–‡ä»¶...")
        if not args.auto_clean_old:
            response = input("æ˜¯å¦å¼€å§‹æ¸…ç†æ—§æ–‡ä»¶? [y/N]: ").strip().lower()
            if response != 'y':
                print("è·³è¿‡æ—§æ–‡ä»¶æ¸…ç†")
            else:
                cleaner.clean_old_files(auto_clean=False)
        else:
            cleaner.clean_old_files(auto_clean=True)
    
    print("\nâœ¨ æ¸…ç†å®Œæˆ!")


if __name__ == "__main__":
    main() 