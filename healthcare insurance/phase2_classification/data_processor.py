#!/usr/bin/env python3
"""
RVL-CDIPæ•°æ®é¢„å¤„ç†å™¨
ç­›é€‰åŒ»ä¿ç›¸å…³çš„5ç±»æ–‡æ¡£ï¼šinvoice, letter, email, memo, form
"""

import os
import sys
import pandas as pd
import numpy as np
from pathlib import Path
import shutil
from collections import Counter
import logging
from sklearn.model_selection import train_test_split

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class RVLCDIPProcessor:
    def __init__(self, data_dir):
        self.data_dir = Path(data_dir)
        self.output_dir = self.data_dir / "processed"
        self.output_dir.mkdir(exist_ok=True)
        
        # RVL-CDIP 16ç±»æ˜ å°„
        self.original_classes = {
            0: 'letter',
            1: 'form', 
            2: 'email',
            3: 'handwritten',
            4: 'advertisement',
            5: 'scientific_report',
            6: 'scientific_publication',
            7: 'specification',
            8: 'file_folder',
            9: 'news_article',
            10: 'budget',
            11: 'invoice',
            12: 'presentation',
            13: 'questionnaire',
            14: 'resume',
            15: 'memo'
        }
        
        # åŒ»ä¿ç›¸å…³çš„5ç±»
        self.target_classes = {
            'invoice': 0,     # è´¦å•å‘ç¥¨
            'letter': 1,      # ä¿¡ä»¶
            'email': 2,       # é‚®ä»¶
            'memo': 3,        # å¤‡å¿˜å½•
            'form': 4         # è¡¨å•
        }
        
        # åŸå§‹ç±»åˆ«åˆ°ç›®æ ‡ç±»åˆ«çš„æ˜ å°„
        self.class_mapping = {
            11: 0,  # invoice -> invoice
            0: 1,   # letter -> letter
            2: 2,   # email -> email
            15: 3,  # memo -> memo
            1: 4    # form -> form
        }
        
    def load_labels(self, split='train'):
        """åŠ è½½æ ‡ç­¾æ–‡ä»¶"""
        label_file = self.data_dir / f"{split}.txt"
        if not label_file.exists():
            logger.error(f"æ ‡ç­¾æ–‡ä»¶ä¸å­˜åœ¨: {label_file}")
            return None
            
        # è¯»å–æ ‡ç­¾æ–‡ä»¶
        data = []
        with open(label_file, 'r') as f:
            for line in f:
                parts = line.strip().split()
                if len(parts) >= 2:
                    image_path = parts[0]
                    label = int(parts[1])
                    data.append({'image_path': image_path, 'original_label': label})
        
        df = pd.DataFrame(data)
        logger.info(f"åŠ è½½ {split} æ•°æ®: {len(df)} æ¡è®°å½•")
        return df
    
    def filter_target_classes(self, df):
        """ç­›é€‰ç›®æ ‡ç±»åˆ«"""
        # åªä¿ç•™ç›®æ ‡ç±»åˆ«
        target_original_labels = list(self.class_mapping.keys())
        filtered_df = df[df['original_label'].isin(target_original_labels)].copy()
        
        # æ˜ å°„åˆ°æ–°çš„æ ‡ç­¾
        filtered_df['label'] = filtered_df['original_label'].map(self.class_mapping)
        filtered_df['class_name'] = filtered_df['label'].map({v: k for k, v in self.target_classes.items()})
        
        logger.info(f"ç­›é€‰åæ•°æ®é‡: {len(filtered_df)}")
        
        # ç»Ÿè®¡å„ç±»åˆ«æ•°é‡
        class_counts = filtered_df['class_name'].value_counts()
        logger.info("å„ç±»åˆ«åˆ†å¸ƒ:")
        for class_name, count in class_counts.items():
            logger.info(f"  {class_name}: {count}")
            
        return filtered_df
    
    def create_balanced_subset(self, df, max_per_class=5000):
        """åˆ›å»ºå¹³è¡¡çš„æ•°æ®å­é›†"""
        balanced_data = []
        
        for class_name in self.target_classes.keys():
            class_data = df[df['class_name'] == class_name]
            
            if len(class_data) > max_per_class:
                # éšæœºé‡‡æ ·
                class_subset = class_data.sample(n=max_per_class, random_state=42)
                logger.info(f"{class_name}: ä» {len(class_data)} é‡‡æ ·åˆ° {max_per_class}")
            else:
                class_subset = class_data
                logger.info(f"{class_name}: ä¿ç•™å…¨éƒ¨ {len(class_data)} æ¡")
            
            balanced_data.append(class_subset)
        
        balanced_df = pd.concat(balanced_data, ignore_index=True)
        balanced_df = balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)  # æ‰“ä¹±é¡ºåº
        
        logger.info(f"å¹³è¡¡åæ€»æ•°æ®é‡: {len(balanced_df)}")
        return balanced_df
    
    def copy_images(self, df, split_name):
        """å¤åˆ¶ç­›é€‰åçš„å›¾ç‰‡åˆ°æ–°ç›®å½•"""
        target_dir = self.output_dir / "images" / split_name
        target_dir.mkdir(parents=True, exist_ok=True)
        
        success_count = 0
        fail_count = 0
        
        for idx, row in df.iterrows():
            src_path = self.data_dir / "images" / row['image_path']
            if src_path.exists():
                # ä½¿ç”¨æ–°çš„æ–‡ä»¶åï¼šç±»åˆ«_åŸå§‹æ–‡ä»¶å
                new_filename = f"{row['class_name']}_{Path(row['image_path']).name}"
                dst_path = target_dir / new_filename
                
                try:
                    shutil.copy2(src_path, dst_path)
                    # æ›´æ–°DataFrameä¸­çš„è·¯å¾„
                    df.at[idx, 'new_image_path'] = f"{split_name}/{new_filename}"
                    success_count += 1
                except Exception as e:
                    logger.error(f"å¤åˆ¶æ–‡ä»¶å¤±è´¥ {src_path}: {e}")
                    fail_count += 1
            else:
                logger.warning(f"æºæ–‡ä»¶ä¸å­˜åœ¨: {src_path}")
                fail_count += 1
        
        logger.info(f"å›¾ç‰‡å¤åˆ¶å®Œæˆ - æˆåŠŸ: {success_count}, å¤±è´¥: {fail_count}")
        return df
    
    def save_processed_data(self, train_df, val_df, test_df):
        """ä¿å­˜å¤„ç†åçš„æ•°æ®"""
        # ä¿å­˜CSVæ–‡ä»¶
        train_df.to_csv(self.output_dir / "train.csv", index=False)
        val_df.to_csv(self.output_dir / "val.csv", index=False)
        test_df.to_csv(self.output_dir / "test.csv", index=False)
        
        # ä¿å­˜ç±»åˆ«ä¿¡æ¯
        class_info = {
            'target_classes': self.target_classes,
            'class_mapping': self.class_mapping,
            'original_classes': self.original_classes
        }
        
        import json
        with open(self.output_dir / "class_info.json", 'w', encoding='utf-8') as f:
            json.dump(class_info, f, indent=2, ensure_ascii=False)
        
        # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
        stats = {
            'total_train': len(train_df),
            'total_val': len(val_df),
            'total_test': len(test_df),
            'train_distribution': train_df['class_name'].value_counts().to_dict(),
            'val_distribution': val_df['class_name'].value_counts().to_dict(),
            'test_distribution': test_df['class_name'].value_counts().to_dict()
        }
        
        with open(self.output_dir / "statistics.json", 'w', encoding='utf-8') as f:
            json.dump(stats, f, indent=2, ensure_ascii=False)
        
        logger.info("æ•°æ®ä¿å­˜å®Œæˆ")
        logger.info(f"è®­ç»ƒé›†: {len(train_df)}")
        logger.info(f"éªŒè¯é›†: {len(val_df)}")
        logger.info(f"æµ‹è¯•é›†: {len(test_df)}")
    
    def process(self, max_per_class=5000):
        """ä¸»å¤„ç†æµç¨‹"""
        logger.info("å¼€å§‹å¤„ç†RVL-CDIPæ•°æ®é›†...")
        
        # åŠ è½½åŸå§‹æ•°æ®
        train_df = self.load_labels('train')
        val_df = self.load_labels('val')
        test_df = self.load_labels('test')
        
        if any(df is None for df in [train_df, val_df, test_df]):
            logger.error("åŠ è½½æ•°æ®å¤±è´¥")
            return False
        
        # ç­›é€‰ç›®æ ‡ç±»åˆ«
        train_filtered = self.filter_target_classes(train_df)
        val_filtered = self.filter_target_classes(val_df)
        test_filtered = self.filter_target_classes(test_df)
        
        # åˆ›å»ºå¹³è¡¡å­é›†
        train_balanced = self.create_balanced_subset(train_filtered, max_per_class)
        val_balanced = self.create_balanced_subset(val_filtered, max_per_class//5)
        test_balanced = self.create_balanced_subset(test_filtered, max_per_class//5)
        
        # å¤åˆ¶å›¾ç‰‡ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if (self.data_dir / "images").exists():
            logger.info("å¤åˆ¶è®­ç»ƒå›¾ç‰‡...")
            train_balanced = self.copy_images(train_balanced, "train")
            logger.info("å¤åˆ¶éªŒè¯å›¾ç‰‡...")
            val_balanced = self.copy_images(val_balanced, "val")
            logger.info("å¤åˆ¶æµ‹è¯•å›¾ç‰‡...")
            test_balanced = self.copy_images(test_balanced, "test")
        else:
            logger.warning("å›¾ç‰‡ç›®å½•ä¸å­˜åœ¨ï¼Œè·³è¿‡å›¾ç‰‡å¤åˆ¶")
        
        # ä¿å­˜å¤„ç†åçš„æ•°æ®
        self.save_processed_data(train_balanced, val_balanced, test_balanced)
        
        logger.info("ğŸ‰ æ•°æ®å¤„ç†å®Œæˆï¼")
        return True

def main():
    """ä¸»å‡½æ•°"""
    # è·å–æ•°æ®ç›®å½•
    base_dir = Path(__file__).parent.parent
    rvl_cdip_dir = base_dir / "data" / "rvl_cdip"
    
    if not rvl_cdip_dir.exists():
        logger.error(f"RVL-CDIPæ•°æ®ç›®å½•ä¸å­˜åœ¨: {rvl_cdip_dir}")
        logger.error("è¯·å…ˆè¿è¡Œ python scripts/download_datasets.py")
        return False
    
    # åˆ›å»ºå¤„ç†å™¨å¹¶å¤„ç†æ•°æ®
    processor = RVLCDIPProcessor(rvl_cdip_dir)
    success = processor.process(max_per_class=5000)
    
    if success:
        logger.info("æ•°æ®é¢„å¤„ç†æˆåŠŸå®Œæˆï¼")
        logger.info(f"å¤„ç†åçš„æ•°æ®ä¿å­˜åœ¨: {processor.output_dir}")
    else:
        logger.error("æ•°æ®é¢„å¤„ç†å¤±è´¥")
    
    return success

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1) 