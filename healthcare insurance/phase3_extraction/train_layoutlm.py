#!/usr/bin/env python3
"""
LayoutLMv3è®­ç»ƒè„šæœ¬
ç”¨äºåŒ»ä¿è¡¨å•å…³é”®ä¿¡æ¯æŠ½å–ä»»åŠ¡
"""

import os
import sys
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from transformers import (
    LayoutLMv3TokenizerFast, LayoutLMv3Processor, LayoutLMv3ForTokenClassification,
    AdamW, get_scheduler, AutoConfig
)
import json
import pandas as pd
import numpy as np
from pathlib import Path
from PIL import Image
import logging
from tqdm import tqdm
from sklearn.metrics import f1_score, classification_report
from seqeval.metrics import accuracy_score, f1_score as seq_f1_score, classification_report as seq_classification_report
import matplotlib.pyplot as plt
import seaborn as sns

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class LayoutLMDataset(Dataset):
    """LayoutLMv3æ•°æ®é›†"""
    
    def __init__(self, data_file, images_dir, processor, max_length=512):
        with open(data_file, 'r', encoding='utf-8') as f:
            self.data = json.load(f)
        
        self.images_dir = Path(images_dir)
        self.processor = processor
        self.max_length = max_length
        
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        sample = self.data[idx]
        
        # åŠ è½½å›¾ç‰‡
        image_path = self.images_dir / sample['image_path']
        try:
            image = Image.open(image_path).convert('RGB')
        except Exception as e:
            logger.warning(f"æ— æ³•åŠ è½½å›¾ç‰‡ {image_path}: {e}")
            # åˆ›å»ºç©ºç™½å›¾ç‰‡ä½œä¸ºæ›¿ä»£
            image = Image.new('RGB', (224, 224), color='white')
        
        # è·å–å•è¯ã€è¾¹ç•Œæ¡†å’Œæ ‡ç­¾
        words = sample['words']
        boxes = sample['boxes']
        labels = sample['labels']
        
        # ä½¿ç”¨processorå¤„ç†
        encoding = self.processor(
            image,
            words,
            boxes=boxes,
            word_labels=labels,
            truncation=True,
            padding='max_length',
            max_length=self.max_length,
            return_tensors='pt'
        )
        
        # ç§»é™¤batchç»´åº¦
        for key in encoding:
            encoding[key] = encoding[key].squeeze(0)
        
        return encoding

class MedicalFieldExtractor:
    """åŒ»ç–—å­—æ®µæŠ½å–å™¨"""
    
    def __init__(self, label_map, medical_fields):
        self.label_map = label_map
        self.id2label = {v: k for k, v in label_map.items()}
        self.medical_fields = medical_fields
        
    def extract_entities(self, words, boxes, predictions):
        """ä»é¢„æµ‹ç»“æœä¸­æŠ½å–å®ä½“"""
        entities = {}
        current_entity = None
        current_words = []
        current_label = None
        
        for i, (word, box, pred_id) in enumerate(zip(words, boxes, predictions)):
            pred_label = self.id2label.get(pred_id, 'O')
            
            if pred_label.startswith('B-'):
                # å¼€å§‹æ–°å®ä½“
                if current_entity and current_words:
                    entities[current_entity] = {
                        'text': ' '.join(current_words),
                        'words': current_words.copy(),
                        'boxes': [boxes[i-len(current_words):i]],
                        'label': current_label
                    }
                
                current_entity = pred_label[2:]
                current_label = pred_label
                current_words = [word]
                
            elif pred_label.startswith('I-') and current_entity == pred_label[2:]:
                # ç»§ç»­å½“å‰å®ä½“
                current_words.append(word)
                
            else:
                # ç»“æŸå½“å‰å®ä½“
                if current_entity and current_words:
                    entities[current_entity] = {
                        'text': ' '.join(current_words),
                        'words': current_words.copy(),
                        'boxes': [boxes[i-len(current_words):i]],
                        'label': current_label
                    }
                
                current_entity = None
                current_words = []
                current_label = None
        
        # å¤„ç†æœ€åä¸€ä¸ªå®ä½“
        if current_entity and current_words:
            entities[current_entity] = {
                'text': ' '.join(current_words),
                'words': current_words.copy(),
                'boxes': [boxes[-len(current_words):]],
                'label': current_label
            }
        
        return entities
    
    def map_to_medical_fields(self, entities):
        """å°†å®ä½“æ˜ å°„åˆ°åŒ»ç–—å­—æ®µ"""
        medical_info = {}
        
        for entity_type, entity_data in entities.items():
            text = entity_data['text'].lower()
            
            # æ£€æŸ¥æ˜¯å¦åŒ¹é…åŒ»ç–—å­—æ®µ
            for field_name, keywords in self.medical_fields.items():
                for keyword in keywords:
                    if keyword in text:
                        medical_info[field_name] = entity_data['text']
                        break
        
        return medical_info

class LayoutLMTrainer:
    """LayoutLMv3è®­ç»ƒå™¨"""
    
    def __init__(self, config):
        self.config = config
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        logger.info(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir = Path(config['output_dir'])
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # åŠ è½½æ ‡ç­¾ä¿¡æ¯
        with open(config['label_info_path'], 'r', encoding='utf-8') as f:
            self.label_info = json.load(f)
        
        self.label_map = self.label_info['label_map']
        self.id2label = self.label_info['id2label']
        self.num_labels = self.label_info['num_labels']
        
        # åˆå§‹åŒ–processor
        self.processor = LayoutLMv3Processor.from_pretrained(config['model_name'])
        
        # åŒ»ç–—å­—æ®µæŠ½å–å™¨
        self.field_extractor = MedicalFieldExtractor(
            self.label_map, 
            self.label_info['medical_fields']
        )
        
    def create_data_loaders(self):
        """åˆ›å»ºæ•°æ®åŠ è½½å™¨"""
        logger.info("åˆ›å»ºæ•°æ®åŠ è½½å™¨...")
        
        # è®­ç»ƒé›†
        train_dataset = LayoutLMDataset(
            data_file=self.config['train_data'],
            images_dir=self.config['images_dir'],
            processor=self.processor,
            max_length=self.config['max_length']
        )
        
        # éªŒè¯é›†
        val_dataset = LayoutLMDataset(
            data_file=self.config['val_data'],
            images_dir=self.config['images_dir'],
            processor=self.processor,
            max_length=self.config['max_length']
        )
        
        # æ•°æ®åŠ è½½å™¨
        train_loader = DataLoader(
            train_dataset,
            batch_size=self.config['batch_size'],
            shuffle=True,
            num_workers=2,
            pin_memory=True
        )
        
        val_loader = DataLoader(
            val_dataset,
            batch_size=self.config['batch_size'],
            shuffle=False,
            num_workers=2,
            pin_memory=True
        )
        
        logger.info(f"è®­ç»ƒé›†å¤§å°: {len(train_dataset)}")
        logger.info(f"éªŒè¯é›†å¤§å°: {len(val_dataset)}")
        
        return train_loader, val_loader
    
    def create_model(self):
        """åˆ›å»ºæ¨¡å‹"""
        logger.info("åˆ›å»ºLayoutLMv3æ¨¡å‹...")
        
        # é…ç½®
        config = AutoConfig.from_pretrained(self.config['model_name'])
        config.num_labels = self.num_labels
        config.id2label = {int(k): v for k, v in self.id2label.items()}
        config.label2id = {v: int(k) for k, v in self.id2label.items()}
        
        # æ¨¡å‹
        model = LayoutLMv3ForTokenClassification.from_pretrained(
            self.config['model_name'],
            config=config
        )
        model.to(self.device)
        
        return model
    
    def create_optimizer_and_scheduler(self, model, num_training_steps):
        """åˆ›å»ºä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨"""
        # ä¼˜åŒ–å™¨
        optimizer = AdamW(
            model.parameters(),
            lr=self.config['learning_rate'],
            weight_decay=self.config['weight_decay']
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        scheduler = get_scheduler(
            name="linear",
            optimizer=optimizer,
            num_warmup_steps=self.config['warmup_steps'],
            num_training_steps=num_training_steps
        )
        
        return optimizer, scheduler
    
    def train_epoch(self, model, train_loader, optimizer, scheduler):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        model.train()
        total_loss = 0
        
        progress_bar = tqdm(train_loader, desc="è®­ç»ƒ")
        
        for batch in progress_bar:
            # ç§»åŠ¨åˆ°è®¾å¤‡
            batch = {k: v.to(self.device) for k, v in batch.items()}
            
            # å‰å‘ä¼ æ’­
            outputs = model(**batch)
            loss = outputs.loss
            
            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            scheduler.step()
            
            total_loss += loss.item()
            
            # æ›´æ–°è¿›åº¦æ¡
            progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})
        
        avg_loss = total_loss / len(train_loader)
        return avg_loss
    
    def evaluate(self, model, val_loader):
        """è¯„ä¼°æ¨¡å‹"""
        model.eval()
        total_loss = 0
        all_predictions = []
        all_labels = []
        
        with torch.no_grad():
            for batch in tqdm(val_loader, desc="éªŒè¯"):
                batch = {k: v.to(self.device) for k, v in batch.items()}
                
                outputs = model(**batch)
                loss = outputs.loss
                total_loss += loss.item()
                
                # è·å–é¢„æµ‹ç»“æœ
                predictions = torch.argmax(outputs.logits, dim=-1)
                
                # åªè€ƒè™‘æœ‰æ•ˆtoken (attention_mask == 1)
                active_mask = batch['attention_mask'].cpu().numpy() == 1
                
                for i in range(predictions.shape[0]):
                    active_preds = predictions[i][active_mask[i]].cpu().numpy()
                    active_labels = batch['labels'][i][active_mask[i]].cpu().numpy()
                    
                    # è¿‡æ»¤æ‰padding token (label == -100)
                    valid_indices = active_labels != -100
                    if valid_indices.any():
                        all_predictions.append(active_preds[valid_indices])
                        all_labels.append(active_labels[valid_indices])
        
        avg_loss = total_loss / len(val_loader)
        
        # è®¡ç®—F1åˆ†æ•°
        flat_predictions = np.concatenate(all_predictions)
        flat_labels = np.concatenate(all_labels)
        
        f1 = f1_score(flat_labels, flat_predictions, average='weighted')
        
        return avg_loss, f1, all_predictions, all_labels
    
    def save_model(self, model, epoch, f1_score):
        """ä¿å­˜æ¨¡å‹"""
        model_path = self.output_dir / f"layoutlmv3_epoch_{epoch}_f1_{f1_score:.4f}"
        
        # ä¿å­˜æ¨¡å‹å’Œå¤„ç†å™¨
        model.save_pretrained(model_path)
        self.processor.save_pretrained(model_path)
        
        # ä¿å­˜é…ç½®
        config_to_save = {
            'num_labels': self.num_labels,
            'label_info': self.label_info,
            'model_name': self.config['model_name'],
            'epoch': epoch,
            'f1_score': f1_score
        }
        
        with open(model_path / "training_config.json", 'w', encoding='utf-8') as f:
            json.dump(config_to_save, f, indent=2, ensure_ascii=False)
        
        logger.info(f"æ¨¡å‹å·²ä¿å­˜åˆ°: {model_path}")
        return model_path
    
    def plot_label_distribution(self, all_labels, epoch):
        """ç»˜åˆ¶æ ‡ç­¾åˆ†å¸ƒå›¾"""
        # ç»Ÿè®¡æ ‡ç­¾åˆ†å¸ƒ
        flat_labels = np.concatenate(all_labels)
        unique_labels, counts = np.unique(flat_labels, return_counts=True)
        
        # è½¬æ¢ä¸ºæ ‡ç­¾åç§°
        label_names = [self.id2label.get(int(label), f'unknown_{label}') for label in unique_labels]
        
        plt.figure(figsize=(12, 6))
        plt.bar(label_names, counts)
        plt.title(f'Label Distribution - Epoch {epoch}')
        plt.xlabel('Labels')
        plt.ylabel('Count')
        plt.xticks(rotation=45)
        plt.tight_layout()
        
        plt.savefig(self.output_dir / f'label_distribution_epoch_{epoch}.png', 
                   dpi=300, bbox_inches='tight')
        plt.close()
        
    def test_medical_field_extraction(self, model, test_samples=5):
        """æµ‹è¯•åŒ»ç–—å­—æ®µæŠ½å–åŠŸèƒ½"""
        logger.info("æµ‹è¯•åŒ»ç–—å­—æ®µæŠ½å–...")
        
        model.eval()
        
        # åˆ›å»ºæµ‹è¯•æ ·æœ¬
        test_words = ["Provider:", "Dr.", "John", "Smith", "Phone:", "555-123-4567", "Patient:", "Jane", "Doe"]
        test_boxes = [[50, 50, 120, 80], [130, 50, 160, 80], [170, 50, 220, 80], [230, 50, 290, 80],
                     [50, 100, 110, 130], [120, 100, 250, 130], [50, 150, 120, 180], [130, 150, 180, 180], [190, 150, 240, 180]]
        
        # åˆ›å»ºç©ºç™½å›¾ç‰‡
        image = Image.new('RGB', (300, 200), color='white')
        
        # å¤„ç†
        encoding = self.processor(
            image,
            test_words,
            boxes=test_boxes,
            truncation=True,
            padding='max_length',
            max_length=self.config['max_length'],
            return_tensors='pt'
        )
        
        # ç§»åŠ¨åˆ°è®¾å¤‡
        encoding = {k: v.to(self.device) for k, v in encoding.items()}
        
        with torch.no_grad():
            outputs = model(**encoding)
            predictions = torch.argmax(outputs.logits, dim=-1)
            
            # è·å–æœ‰æ•ˆé¢„æµ‹
            active_mask = encoding['attention_mask'][0].cpu().numpy() == 1
            active_predictions = predictions[0][active_mask].cpu().numpy()
            
            # æŠ½å–å®ä½“
            entities = self.field_extractor.extract_entities(
                test_words, test_boxes, active_predictions[:len(test_words)]
            )
            
            # æ˜ å°„åˆ°åŒ»ç–—å­—æ®µ
            medical_info = self.field_extractor.map_to_medical_fields(entities)
            
            logger.info("æµ‹è¯•ç»“æœ:")
            logger.info(f"å®ä½“: {entities}")
            logger.info(f"åŒ»ç–—ä¿¡æ¯: {medical_info}")
    
    def train(self):
        """ä¸»è®­ç»ƒæµç¨‹"""
        logger.info("å¼€å§‹è®­ç»ƒLayoutLMv3...")
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_loader, val_loader = self.create_data_loaders()
        
        # åˆ›å»ºæ¨¡å‹
        model = self.create_model()
        
        # è®¡ç®—è®­ç»ƒæ­¥æ•°
        num_training_steps = len(train_loader) * self.config['epochs']
        
        # åˆ›å»ºä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
        optimizer, scheduler = self.create_optimizer_and_scheduler(model, num_training_steps)
        
        # è®­ç»ƒå¾ªç¯
        best_f1 = 0
        best_model_path = None
        
        for epoch in range(self.config['epochs']):
            logger.info(f"Epoch {epoch + 1}/{self.config['epochs']}")
            
            # è®­ç»ƒ
            train_loss = self.train_epoch(model, train_loader, optimizer, scheduler)
            
            # éªŒè¯
            val_loss, val_f1, val_predictions, val_labels = self.evaluate(model, val_loader)
            
            logger.info(f"è®­ç»ƒæŸå¤±: {train_loss:.4f}")
            logger.info(f"éªŒè¯æŸå¤±: {val_loss:.4f}, éªŒè¯F1: {val_f1:.4f}")
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_f1 > best_f1:
                best_f1 = val_f1
                best_model_path = self.save_model(model, epoch + 1, val_f1)
            
            # ç»˜åˆ¶æ ‡ç­¾åˆ†å¸ƒ
            self.plot_label_distribution(val_labels, epoch + 1)
            
            # æµ‹è¯•åŒ»ç–—å­—æ®µæŠ½å–
            self.test_medical_field_extraction(model)
        
        logger.info(f"ğŸ‰ è®­ç»ƒå®Œæˆï¼æœ€ä½³F1åˆ†æ•°: {best_f1:.4f}")
        logger.info(f"æœ€ä½³æ¨¡å‹ä¿å­˜åœ¨: {best_model_path}")
        
        return best_model_path, best_f1

def main():
    """ä¸»å‡½æ•°"""
    # é…ç½®
    base_dir = Path(__file__).parent.parent
    data_dir = base_dir / "data" / "funsd" / "processed"
    
    config = {
        'model_name': 'microsoft/layoutlmv3-base',
        'train_data': str(data_dir / "train.json"),
        'val_data': str(data_dir / "validation.json"),
        'images_dir': str(base_dir / "data" / "funsd"),
        'label_info_path': str(data_dir / "label_info.json"),
        'output_dir': str(base_dir / "models" / "extraction"),
        'batch_size': 8,
        'learning_rate': 1e-5,
        'weight_decay': 0.01,
        'epochs': 5,
        'warmup_steps': 1000,
        'max_length': 512
    }
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    required_files = [
        config['train_data'],
        config['val_data'],
        config['label_info_path']
    ]
    
    for file_path in required_files:
        if not Path(file_path).exists():
            logger.error(f"å¿…éœ€æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            logger.error("è¯·å…ˆè¿è¡ŒFUNSDæ•°æ®é¢„å¤„ç†è„šæœ¬")
            return False
    
    # åˆ›å»ºè®­ç»ƒå™¨å¹¶è®­ç»ƒ
    trainer = LayoutLMTrainer(config)
    best_model_path, best_f1 = trainer.train()
    
    logger.info("è®­ç»ƒæˆåŠŸå®Œæˆï¼")
    return True

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1) 