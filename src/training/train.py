#!/usr/bin/env python3
"""
è®­ç»ƒè„šæœ¬æ¨¡æ¿
ç”¨äºåœ¨Colab GPUç¯å¢ƒä¸­è®­ç»ƒæ·±åº¦å­¦ä¹ æ¨¡å‹
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import json
import os
from pathlib import Path
import wandb
from tqdm import tqdm
import matplotlib.pyplot as plt

def load_config(config_path='configs/train_config.json'):
    """åŠ è½½è®­ç»ƒé…ç½®"""
    with open(config_path, 'r', encoding='utf-8') as f:
        return json.load(f)

def setup_device():
    """è®¾ç½®è®­ç»ƒè®¾å¤‡"""
    if torch.cuda.is_available():
        device = torch.device('cuda')
        print(f"âœ… ä½¿ç”¨GPU: {torch.cuda.get_device_name(0)}")
    else:
        device = torch.device('cpu')
        print("âš ï¸ ä½¿ç”¨CPUè®­ç»ƒ")
    return device

def create_model(config):
    """åˆ›å»ºæ¨¡å‹"""
    model_name = config['model']['name']
    num_classes = config['model']['num_classes']
    
    if model_name == 'resnet50':
        from torchvision.models import resnet50
        model = resnet50(pretrained=config['model']['pretrained'])
        model.fc = nn.Linear(model.fc.in_features, num_classes)
    else:
        # æ·»åŠ å…¶ä»–æ¨¡å‹
        raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹: {model_name}")
    
    return model

def create_optimizer(model, config):
    """åˆ›å»ºä¼˜åŒ–å™¨"""
    optimizer_name = config['training']['optimizer'].lower()
    lr = config['training']['learning_rate']
    
    if optimizer_name == 'adam':
        return optim.Adam(model.parameters(), lr=lr)
    elif optimizer_name == 'sgd':
        return optim.SGD(model.parameters(), lr=lr, momentum=0.9)
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„ä¼˜åŒ–å™¨: {optimizer_name}")

def train_epoch(model, dataloader, criterion, optimizer, device):
    """è®­ç»ƒä¸€ä¸ªepoch"""
    model.train()
    running_loss = 0.0
    correct = 0
    total = 0
    
    pbar = tqdm(dataloader, desc='è®­ç»ƒä¸­')
    for batch_idx, (data, targets) in enumerate(pbar):
        data, targets = data.to(device), targets.to(device)
        
        optimizer.zero_grad()
        outputs = model(data)
        loss = criterion(outputs, targets)
        loss.backward()
        optimizer.step()
        
        running_loss += loss.item()
        _, predicted = outputs.max(1)
        total += targets.size(0)
        correct += predicted.eq(targets).sum().item()
        
        # æ›´æ–°è¿›åº¦æ¡
        pbar.set_postfix({
            'Loss': f'{running_loss/(batch_idx+1):.3f}',
            'Acc': f'{100.*correct/total:.2f}%'
        })
    
    return running_loss / len(dataloader), 100. * correct / total

def validate(model, dataloader, criterion, device):
    """éªŒè¯æ¨¡å‹"""
    model.eval()
    running_loss = 0.0
    correct = 0
    total = 0
    
    with torch.no_grad():
        for data, targets in tqdm(dataloader, desc='éªŒè¯ä¸­'):
            data, targets = data.to(device), targets.to(device)
            outputs = model(data)
            loss = criterion(outputs, targets)
            
            running_loss += loss.item()
            _, predicted = outputs.max(1)
            total += targets.size(0)
            correct += predicted.eq(targets).sum().item()
    
    return running_loss / len(dataloader), 100. * correct / total

def save_checkpoint(model, optimizer, epoch, loss, path):
    """ä¿å­˜æ£€æŸ¥ç‚¹"""
    checkpoint = {
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'loss': loss,
    }
    torch.save(checkpoint, path)
    print(f"âœ… æ£€æŸ¥ç‚¹å·²ä¿å­˜: {path}")

def plot_training_history(train_losses, val_losses, train_accs, val_accs):
    """ç»˜åˆ¶è®­ç»ƒå†å²"""
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))
    
    # æŸå¤±æ›²çº¿
    ax1.plot(train_losses, label='è®­ç»ƒæŸå¤±')
    ax1.plot(val_losses, label='éªŒè¯æŸå¤±')
    ax1.set_xlabel('Epoch')
    ax1.set_ylabel('æŸå¤±')
    ax1.set_title('è®­ç»ƒå’ŒéªŒè¯æŸå¤±')
    ax1.legend()
    ax1.grid(True)
    
    # å‡†ç¡®ç‡æ›²çº¿
    ax2.plot(train_accs, label='è®­ç»ƒå‡†ç¡®ç‡')
    ax2.plot(val_accs, label='éªŒè¯å‡†ç¡®ç‡')
    ax2.set_xlabel('Epoch')
    ax2.set_ylabel('å‡†ç¡®ç‡ (%)')
    ax2.set_title('è®­ç»ƒå’ŒéªŒè¯å‡†ç¡®ç‡')
    ax2.legend()
    ax2.grid(True)
    
    plt.tight_layout()
    plt.savefig('outputs/training_history.png', dpi=300, bbox_inches='tight')
    plt.show()

def main():
    """ä¸»è®­ç»ƒå‡½æ•°"""
    # åŠ è½½é…ç½®
    config = load_config()
    print("ğŸ“‹ è®­ç»ƒé…ç½®:")
    print(json.dumps(config, indent=2, ensure_ascii=False))
    
    # è®¾ç½®è®¾å¤‡
    device = setup_device()
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs('outputs', exist_ok=True)
    os.makedirs('models', exist_ok=True)
    
    # åˆå§‹åŒ–wandbï¼ˆå¯é€‰ï¼‰
    use_wandb = input("æ˜¯å¦ä½¿ç”¨Wandbè®°å½•è®­ç»ƒ? (y/n): ").lower() == 'y'
    if use_wandb:
        wandb.init(project='ai-ml-pipeline', config=config)
    
    # åˆ›å»ºæ¨¡å‹
    model = create_model(config)
    model = model.to(device)
    
    # åˆ›å»ºæŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    criterion = nn.CrossEntropyLoss()
    optimizer = create_optimizer(model, config)
    
    # TODO: åˆ›å»ºæ•°æ®åŠ è½½å™¨
    # train_loader = create_dataloader(config, 'train')
    # val_loader = create_dataloader(config, 'val')
    
    print("âš ï¸ è¯·åœ¨è¿™é‡Œæ·»åŠ æ‚¨çš„æ•°æ®åŠ è½½ä»£ç ")
    print("ğŸ’¡ æç¤º: å–æ¶ˆæ³¨é‡Šä¸Šé¢çš„ä»£ç å¹¶å®ç°create_dataloaderå‡½æ•°")
    
    # è®­ç»ƒå¾ªç¯
    epochs = config['training']['epochs']
    best_val_acc = 0.0
    train_losses, val_losses = [], []
    train_accs, val_accs = [], []
    
    for epoch in range(epochs):
        print(f"\nğŸ“ˆ Epoch {epoch+1}/{epochs}")
        print("-" * 50)
        
        # TODO: å®é™…è®­ç»ƒä»£ç 
        # train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)
        # val_loss, val_acc = validate(model, val_loader, criterion, device)
        
        # ç¤ºä¾‹æ•°æ®ï¼ˆè¯·æ›¿æ¢ä¸ºå®é™…è®­ç»ƒç»“æœï¼‰
        train_loss, train_acc = 0.5, 85.0
        val_loss, val_acc = 0.6, 82.0
        
        train_losses.append(train_loss)
        val_losses.append(val_loss)
        train_accs.append(train_acc)
        val_accs.append(val_acc)
        
        print(f"è®­ç»ƒ - æŸå¤±: {train_loss:.4f}, å‡†ç¡®ç‡: {train_acc:.2f}%")
        print(f"éªŒè¯ - æŸå¤±: {val_loss:.4f}, å‡†ç¡®ç‡: {val_acc:.2f}%")
        
        # è®°å½•åˆ°wandb
        if use_wandb:
            wandb.log({
                'epoch': epoch,
                'train_loss': train_loss,
                'train_acc': train_acc,
                'val_loss': val_loss,
                'val_acc': val_acc
            })
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            save_checkpoint(model, optimizer, epoch, val_loss, 'models/best_model.pth')
        
        # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
        if (epoch + 1) % 5 == 0:
            save_checkpoint(model, optimizer, epoch, val_loss, f'models/checkpoint_epoch_{epoch+1}.pth')
    
    # ç»˜åˆ¶è®­ç»ƒå†å²
    plot_training_history(train_losses, val_losses, train_accs, val_accs)
    
    print(f"\nğŸ‰ è®­ç»ƒå®Œæˆï¼æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.2f}%")
    
    if use_wandb:
        wandb.finish()

if __name__ == "__main__":
    main() 