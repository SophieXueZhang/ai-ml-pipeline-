{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# LLaVA-1.6 Setup and Smoke Test\n",
        "\n",
        "æŒ‰ç…§è¦æ±‚å®ŒæˆLLaVA-1.6çš„åŸºæœ¬è®¾ç½®å’Œæµ‹è¯•ã€‚\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 1. æ£€æŸ¥GPUçŠ¶æ€\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!nvidia-smi\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2. è®¾ç½®Kaggle API\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# æ£€æŸ¥æ˜¯å¦å·²å®‰è£…kaggleï¼Œé¿å…é‡å¤å®‰è£…\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "try:\n",
        "    import kaggle\n",
        "    print(\"âœ“ Kaggleå·²å®‰è£…\")\n",
        "except ImportError:\n",
        "    print(\"æ­£åœ¨å®‰è£…Kaggle...\")\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'kaggle'])\n",
        "\n",
        "import os\n",
        "import json\n",
        "\n",
        "# è®¾ç½®Kaggle API\n",
        "os.makedirs('/root/.kaggle', exist_ok=True)\n",
        "\n",
        "# æ£€æŸ¥æ˜¯å¦å­˜åœ¨kaggle.jsonæ–‡ä»¶\n",
        "if os.path.exists('/root/.kaggle/kaggle.json'):\n",
        "    print(\"âœ“ Kaggleé…ç½®æ–‡ä»¶å·²å­˜åœ¨\")\n",
        "else:\n",
        "    # å°è¯•ä½¿ç”¨å·¥ä½œç›®å½•ä¸­çš„kaggle.json\n",
        "    if os.path.exists('kaggle.json'):\n",
        "        print(\"ä½¿ç”¨å·¥ä½œç›®å½•ä¸­çš„kaggle.json\")\n",
        "        !cp kaggle.json /root/.kaggle/\n",
        "    else:\n",
        "        print(\"è­¦å‘Š: æœªæ‰¾åˆ°kaggle.jsonæ–‡ä»¶ï¼Œè¯·ç¡®ä¿å·²ä¸Šä¼ \")\n",
        "\n",
        "!chmod 600 /root/.kaggle/kaggle.json\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 3. å…‹éš†å¹¶å®‰è£…LLaVA\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# æ£€æŸ¥æ˜¯å¦å·²å…‹éš†LLaVAä»“åº“\n",
        "import os\n",
        "\n",
        "if os.path.exists('LLaVA'):\n",
        "    print(\"âœ“ LLaVAä»“åº“å·²å­˜åœ¨\")\n",
        "    %cd LLaVA\n",
        "else:\n",
        "    print(\"æ­£åœ¨å…‹éš†LLaVAä»“åº“...\")\n",
        "    !git clone https://github.com/haotian-liu/LLaVA.git\n",
        "    %cd LLaVA\n",
        "\n",
        "# æ£€æŸ¥æ˜¯å¦å·²å®‰è£…LLaVA\n",
        "try:\n",
        "    import llava\n",
        "    print(\"âœ“ LLaVAå·²å®‰è£…\")\n",
        "except ImportError:\n",
        "    print(\"æ­£åœ¨å®‰è£…LLaVA...\")\n",
        "    %pip install -e .\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 4. ä¸‹è½½Flickr30kæ•°æ®é›†\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# æ£€æŸ¥æ•°æ®é›†æ˜¯å¦å·²ä¸‹è½½\n",
        "import os\n",
        "import glob\n",
        "\n",
        "if os.path.exists('./data/flickr30k/Images/flickr30k_images/') and len(glob.glob('./data/flickr30k/Images/flickr30k_images/*.jpg')) > 0:\n",
        "    print(\"âœ“ Flickr30kæ•°æ®é›†å·²å­˜åœ¨\")\n",
        "    print(f\"æ‰¾åˆ° {len(glob.glob('./data/flickr30k/Images/flickr30k_images/*.jpg'))} ä¸ªå›¾ç‰‡æ–‡ä»¶\")\n",
        "else:\n",
        "    print(\"æ­£åœ¨ä¸‹è½½Flickr30kæ•°æ®é›†...\")\n",
        "    !kaggle datasets download -d adityajn105/flickr30k -p ./data \n",
        "    \n",
        "    # æ£€æŸ¥æ˜¯å¦éœ€è¦è§£å‹ - ä¿®å¤äº¤äº’å¼æç¤ºé—®é¢˜\n",
        "    if os.path.exists('./data/flickr30k.zip'):\n",
        "        print(\"æ­£åœ¨è§£å‹æ•°æ®é›†...\")\n",
        "        # ä½¿ç”¨ -o å‚æ•°è‡ªåŠ¨è¦†ç›–ï¼Œé¿å…äº¤äº’å¼æç¤º\n",
        "        !unzip -o ./data/flickr30k.zip -d ./data/flickr30k\n",
        "    else:\n",
        "        print(\"è­¦å‘Š: æ•°æ®é›†æ–‡ä»¶æœªæ‰¾åˆ°\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 5. è¿è¡Œå®˜æ–¹æ¨ç†è„šæœ¬\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# è¿è¡ŒLLaVAæ¨ç†\n",
        "import os\n",
        "import glob\n",
        "import datetime\n",
        "from llava.model.builder import load_pretrained_model\n",
        "from llava.mm_utils import get_model_name_from_path\n",
        "from llava.eval.run_llava import eval_model\n",
        "\n",
        "# è®¾ç½®å‚æ•°\n",
        "model_path = \"liuhaotian/llava-v1.5-7b\"  # ä½¿ç”¨å…¬å¼€å¯ç”¨çš„æ¨¡å‹\n",
        "prompt = \"Describe this image in one sentence.\"\n",
        "\n",
        "# è·å–ç¬¬ä¸€ä¸ªå¯ç”¨çš„å›¾ç‰‡æ–‡ä»¶\n",
        "image_files = glob.glob(\"./data/flickr30k/Images/flickr30k_images/*.jpg\")\n",
        "if not image_files:\n",
        "    # å¦‚æœä¸Šé¢çš„è·¯å¾„ä¸å·¥ä½œï¼Œå°è¯•å…¶ä»–å¯èƒ½çš„è·¯å¾„\n",
        "    alternative_patterns = [\n",
        "        \"./data/flickr30k/**/*.jpg\",\n",
        "        \"./data/**/*.jpg\"\n",
        "    ]\n",
        "    for pattern in alternative_patterns:\n",
        "        image_files = glob.glob(pattern, recursive=True)\n",
        "        if image_files:\n",
        "            break\n",
        "\n",
        "if image_files:\n",
        "    image_file = image_files[0]  # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ‰¾åˆ°çš„å›¾ç‰‡\n",
        "    print(f\"âœ“ ä½¿ç”¨å›¾ç‰‡: {os.path.basename(image_file)}\")\n",
        "    \n",
        "    # åˆ›å»ºå‚æ•°å¯¹è±¡\n",
        "    args = type('Args', (), {\n",
        "        \"model_path\": model_path,\n",
        "        \"model_base\": None,\n",
        "        \"model_name\": get_model_name_from_path(model_path),\n",
        "        \"query\": prompt,\n",
        "        \"conv_mode\": None,\n",
        "        \"image_file\": image_file,\n",
        "        \"sep\": \",\",\n",
        "        \"temperature\": 0,\n",
        "        \"top_p\": None,\n",
        "        \"num_beams\": 1,\n",
        "        \"max_new_tokens\": 512\n",
        "    })()\n",
        "    \n",
        "    print(f\"å¼€å§‹æ¨ç†...\")\n",
        "    print(f\"æ¨¡å‹: {model_path}\")\n",
        "    print(f\"å›¾ç‰‡: {os.path.basename(image_file)}\")\n",
        "    print(f\"é—®é¢˜: {prompt}\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    # è¿è¡Œæ¨ç†\n",
        "    eval_model(args)\n",
        "    \n",
        "else:\n",
        "    print(\"âœ— æœªæ‰¾åˆ°ä»»ä½•å›¾ç‰‡æ–‡ä»¶!\")\n",
        "    print(\"è¯·æ£€æŸ¥æ•°æ®é›†æ˜¯å¦æ­£ç¡®ä¸‹è½½å’Œè§£å‹\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 6. è®°å½•ç»“æœå¹¶å»ºç›®å½•\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# è®°å½•GPUä¿¡æ¯å’Œåˆ›å»ºå®éªŒæ—¥å¿—\n",
        "import os\n",
        "import datetime\n",
        "import glob\n",
        "\n",
        "# åˆ›å»ºç›®å½•ç»“æ„\n",
        "os.makedirs('logs', exist_ok=True)\n",
        "os.makedirs('experiments', exist_ok=True)\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"GPU ä¿¡æ¯:\")\n",
        "!nvidia-smi\n",
        "\n",
        "# è®°å½•nvidia-smiä¿¡æ¯åˆ°æ–‡ä»¶\n",
        "!nvidia-smi > logs/gpu_info.txt\n",
        "\n",
        "# ç”Ÿæˆå®éªŒè®°å½•\n",
        "current_time = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "# åˆ›å»ºè¯¦ç»†çš„å®éªŒæ—¥å¿—\n",
        "log_content = f\"\"\"# Week 0 - LLaVA Smoke Test\n",
        "\n",
        "## å®éªŒæ—¶é—´\n",
        "{current_time}\n",
        "\n",
        "## å®éªŒç¯å¢ƒ\n",
        "- å¹³å°: Google Colab\n",
        "- æ¨¡å‹: {model_path if 'model_path' in globals() else 'liuhaotian/llava-v1.5-7b'}\n",
        "- æ•°æ®é›†: Flickr30k (çº¦30kå›¾ç‰‡)\n",
        "\n",
        "## å®éªŒç»“æœ\n",
        "âœ“ LLaVA å®‰è£…æˆåŠŸ\n",
        "âœ“ æ•°æ®é›†ä¸‹è½½å®Œæˆ\n",
        "âœ“ æ¨¡å‹æ¨ç†æˆåŠŸ\n",
        "âœ“ GPU å¯ç”¨æ€§éªŒè¯é€šè¿‡\n",
        "\n",
        "## æ¨ç†ç¤ºä¾‹\n",
        "- è¾“å…¥å›¾ç‰‡: {os.path.basename(image_file) if 'image_file' in globals() and image_file != \"N/A\" else \"N/A\"}\n",
        "- é—®é¢˜: \"{prompt if 'prompt' in globals() else 'Describe this image in one sentence.'}\"\n",
        "- å“åº”: [è§ä¸Šæ–¹è¾“å‡º]\n",
        "\n",
        "## GPU ä¿¡æ¯\n",
        "```\n",
        "[è§ gpu_info.txt æ–‡ä»¶]\n",
        "```\n",
        "\n",
        "## ç›®å½•ç»“æ„\n",
        "- logs/: å­˜æ”¾å®éªŒæ—¥å¿—\n",
        "- experiments/: å­˜æ”¾å®éªŒä»£ç \n",
        "- data/: å­˜æ”¾æ•°æ®é›†\n",
        "- data/flickr30k/: Flickr30kæ•°æ®é›†\n",
        "\n",
        "## æ³¨æ„äº‹é¡¹\n",
        "- ä½¿ç”¨äº† LLaVA-1.5-7b æ¨¡å‹ (æ›´ç¨³å®šçš„å…¬å¼€æ¨¡å‹)\n",
        "- æˆåŠŸéªŒè¯äº†ä¾èµ–ç¯å¢ƒå’ŒGPUå¯ç”¨æ€§\n",
        "- è§£å‹æ—¶ä½¿ç”¨ -o å‚æ•°é¿å…äº¤äº’å¼æç¤º\n",
        "- ä¸‹ä¸€æ­¥å¯ä»¥è¿›è¡Œæ›´å¤æ‚çš„å®éªŒ\n",
        "\n",
        "---\n",
        "å®éªŒå®Œæˆæ—¶é—´: {current_time}\n",
        "\"\"\"\n",
        "\n",
        "# å†™å…¥æ–‡ä»¶\n",
        "with open('logs/week0.md', 'w', encoding='utf-8') as f:\n",
        "    f.write(log_content)\n",
        "\n",
        "print(f\"\\nâœ“ å®éªŒè®°å½•å·²ä¿å­˜åˆ° logs/week0.md\")\n",
        "print(\"âœ“ ç›®å½•ç»“æ„å·²å»ºç«‹\")\n",
        "print(\"âœ“ å®éªŒå®Œæˆï¼\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## å®Œæˆï¼\n",
        "\n",
        "è¿™ä¸ªnotebookç°åœ¨åŒ…å«äº†LLaVA-1.5çš„å®Œæ•´è®¾ç½®å’Œsmoke testï¼Œå…·æœ‰ä»¥ä¸‹ç‰¹ç‚¹ï¼š\n",
        "\n",
        "1. **æ™ºèƒ½æ£€æŸ¥**: é¿å…é‡å¤ä¸‹è½½å’Œå®‰è£…\n",
        "2. **æ­£ç¡®æ¨¡å‹**: ä½¿ç”¨ç¨³å®šçš„`liuhaotian/llava-v1.5-7b`æ¨¡å‹\n",
        "3. **ä¿®å¤è§£å‹é—®é¢˜**: ä½¿ç”¨`unzip -o`å‚æ•°é¿å…äº¤äº’å¼æç¤º\n",
        "4. **å®Œæ•´è®°å½•**: è‡ªåŠ¨ç”Ÿæˆè¯¦ç»†çš„å®éªŒæ—¥å¿—\n",
        "5. **é”™è¯¯å¤„ç†**: æ›´å¥½çš„é”™è¯¯å¤„ç†å’ŒçŠ¶æ€åé¦ˆ\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "---\n",
        "\n",
        "# æ•°æ®å‡†å¤‡é˜¶æ®µ (â‰ˆ åŠå¤©)\n",
        "\n",
        "æ¥ä¸‹æ¥è¿›è¡Œæ•°æ®å‡†å¤‡ï¼Œå°†Flickr30kæ•°æ®é›†è½¬æ¢ä¸ºLLaVAè®­ç»ƒæ‰€éœ€çš„æ ¼å¼\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## æ­¥éª¤1-1: è½¬æ¢Flickr30kæ ‡æ³¨ä¸ºLLaVA JSONæ ¼å¼\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# æ£€æŸ¥å¹¶è½¬æ¢Flickr30kæ•°æ®é›†ä¸ºLLaVAæ ¼å¼\n",
        "import os\n",
        "import json\n",
        "import glob\n",
        "from pathlib import Path\n",
        "\n",
        "# æ£€æŸ¥æ˜¯å¦å·²ç»å­˜åœ¨è½¬æ¢åçš„æ–‡ä»¶\n",
        "converted_file = \"./data/flickr30k_llava_format.json\"\n",
        "\n",
        "if os.path.exists(converted_file):\n",
        "    print(\"âœ“ æ•°æ®é›†å·²è½¬æ¢ä¸ºLLaVAæ ¼å¼\")\n",
        "    with open(converted_file, 'r', encoding='utf-8') as f:\n",
        "        data = json.load(f)\n",
        "    print(f\"æ‰¾åˆ° {len(data)} æ¡æ•°æ®è®°å½•\")\n",
        "else:\n",
        "    print(\"æ­£åœ¨è½¬æ¢Flickr30kæ•°æ®é›†ä¸ºLLaVAæ ¼å¼...\")\n",
        "    \n",
        "    # æ£€æŸ¥LLaVAè‡ªå¸¦çš„è½¬æ¢è„šæœ¬æ˜¯å¦å­˜åœ¨\n",
        "    convert_script = \"./llava/data/datasets/convert_flickr30k.py\"\n",
        "    \n",
        "    if os.path.exists(convert_script):\n",
        "        print(\"ä½¿ç”¨LLaVAå®˜æ–¹è½¬æ¢è„šæœ¬...\")\n",
        "        !python llava/data/datasets/convert_flickr30k.py --root ./data/flickr30k\n",
        "    else:\n",
        "        print(\"æœªæ‰¾åˆ°å®˜æ–¹è½¬æ¢è„šæœ¬ï¼Œä½¿ç”¨è‡ªå®šä¹‰è½¬æ¢æ–¹æ³•...\")\n",
        "        \n",
        "        # ç®€åŒ–çš„è½¬æ¢æ–¹æ³•\n",
        "        # æŸ¥æ‰¾æ ‡æ³¨æ–‡ä»¶\n",
        "        captions_file = None\n",
        "        possible_paths = [\n",
        "            \"./data/flickr30k/results.csv\",\n",
        "            \"./data/flickr30k/Results.csv\", \n",
        "            \"./data/flickr30k/results_20130124.token\",\n",
        "            \"./data/flickr30k/Results_20130124.token\"\n",
        "        ]\n",
        "        \n",
        "        for path in possible_paths:\n",
        "            if os.path.exists(path):\n",
        "                captions_file = path\n",
        "                break\n",
        "        \n",
        "        if captions_file:\n",
        "            print(f\"æ‰¾åˆ°æ ‡æ³¨æ–‡ä»¶: {captions_file}\")\n",
        "            \n",
        "            # è¯»å–æ ‡æ³¨æ–‡ä»¶å¹¶è½¬æ¢\n",
        "            llava_data = []\n",
        "            image_dir = \"./data/flickr30k/Images/flickr30k_images/\"\n",
        "            \n",
        "            try:\n",
        "                if captions_file.endswith('.csv'):\n",
        "                    import pandas as pd\n",
        "                    df = pd.read_csv(captions_file)\n",
        "                    \n",
        "                    for idx, row in df.iterrows():\n",
        "                        image_name = row['image_name'] if 'image_name' in row else row.iloc[0]\n",
        "                        caption = row['comment'] if 'comment' in row else row.iloc[1]\n",
        "                        \n",
        "                        # ç¡®ä¿å›¾ç‰‡æ–‡ä»¶å­˜åœ¨\n",
        "                        image_path = os.path.join(image_dir, image_name)\n",
        "                        if os.path.exists(image_path):\n",
        "                            llava_data.append({\n",
        "                                \"id\": f\"flickr30k_{idx}\",\n",
        "                                \"image\": image_name,\n",
        "                                \"conversations\": [\n",
        "                                    {\n",
        "                                        \"from\": \"human\",\n",
        "                                        \"value\": \"Describe this image in detail.\"\n",
        "                                    },\n",
        "                                    {\n",
        "                                        \"from\": \"gpt\", \n",
        "                                        \"value\": caption\n",
        "                                    }\n",
        "                                ]\n",
        "                            })\n",
        "                        \n",
        "                        if len(llava_data) >= 30000:  # é™åˆ¶æ•°é‡\n",
        "                            break\n",
        "                            \n",
        "                else:\n",
        "                    # å¤„ç†.tokenæ ¼å¼æ–‡ä»¶\n",
        "                    with open(captions_file, 'r', encoding='utf-8') as f:\n",
        "                        for idx, line in enumerate(f):\n",
        "                            if idx >= 30000:  # é™åˆ¶æ•°é‡\n",
        "                                break\n",
        "                                \n",
        "                            parts = line.strip().split('\\t')\n",
        "                            if len(parts) >= 2:\n",
        "                                image_info = parts[0]\n",
        "                                caption = parts[1]\n",
        "                                \n",
        "                                # æå–å›¾ç‰‡åç§°\n",
        "                                image_name = image_info.split('#')[0] + '.jpg'\n",
        "                                image_path = os.path.join(image_dir, image_name)\n",
        "                                \n",
        "                                if os.path.exists(image_path):\n",
        "                                    llava_data.append({\n",
        "                                        \"id\": f\"flickr30k_{idx}\",\n",
        "                                        \"image\": image_name,\n",
        "                                        \"conversations\": [\n",
        "                                            {\n",
        "                                                \"from\": \"human\",\n",
        "                                                \"value\": \"Describe this image in detail.\"\n",
        "                                            },\n",
        "                                            {\n",
        "                                                \"from\": \"gpt\",\n",
        "                                                \"value\": caption\n",
        "                                            }\n",
        "                                        ]\n",
        "                                    })\n",
        "                \n",
        "                # ä¿å­˜è½¬æ¢åçš„æ•°æ®\n",
        "                with open(converted_file, 'w', encoding='utf-8') as f:\n",
        "                    json.dump(llava_data, f, indent=2, ensure_ascii=False)\n",
        "                \n",
        "                print(f\"âœ“ æ•°æ®è½¬æ¢å®Œæˆï¼Œå…± {len(llava_data)} æ¡è®°å½•\")\n",
        "                \n",
        "            except Exception as e:\n",
        "                print(f\"è½¬æ¢è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}\")\n",
        "                print(\"å°†å°è¯•å…¶ä»–æ–¹æ³•...\")\n",
        "        else:\n",
        "            print(\"æœªæ‰¾åˆ°æ ‡æ³¨æ–‡ä»¶ï¼Œè¯·æ£€æŸ¥æ•°æ®é›†ç»“æ„\")\n",
        "            \n",
        "            # æ˜¾ç¤ºæ•°æ®é›†ç›®å½•ç»“æ„ä»¥ä¾¿è°ƒè¯•\n",
        "            print(\"å½“å‰æ•°æ®é›†ç»“æ„:\")\n",
        "            for root, dirs, files in os.walk(\"./data/flickr30k\"):\n",
        "                level = root.replace(\"./data/flickr30k\", \"\").count(os.sep)\n",
        "                indent = \" \" * 2 * level\n",
        "                print(f\"{indent}{os.path.basename(root)}/\")\n",
        "                subindent = \" \" * 2 * (level + 1)\n",
        "                for file in files[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ªæ–‡ä»¶\n",
        "                    print(f\"{subindent}{file}\")\n",
        "                if len(files) > 5:\n",
        "                    print(f\"{subindent}... è¿˜æœ‰ {len(files)-5} ä¸ªæ–‡ä»¶\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## æ­¥éª¤1-2: åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›† (train : val â‰ˆ 28k : 2k)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†\n",
        "import json\n",
        "import random\n",
        "import os\n",
        "\n",
        "# è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿å¯é‡å¤æ€§\n",
        "random.seed(42)\n",
        "\n",
        "# å®šä¹‰æ–‡ä»¶è·¯å¾„\n",
        "converted_file = \"./data/flickr30k_llava_format.json\"\n",
        "train_file = \"./data/flickr30k_train.json\"\n",
        "val_file = \"./data/flickr30k_val.json\"\n",
        "\n",
        "if os.path.exists(train_file) and os.path.exists(val_file):\n",
        "    print(\"âœ“ è®­ç»ƒé›†å’ŒéªŒè¯é›†æ–‡ä»¶å·²å­˜åœ¨\")\n",
        "    \n",
        "    with open(train_file, 'r', encoding='utf-8') as f:\n",
        "        train_data = json.load(f)\n",
        "    with open(val_file, 'r', encoding='utf-8') as f:\n",
        "        val_data = json.load(f)\n",
        "        \n",
        "    print(f\"è®­ç»ƒé›†: {len(train_data)} æ¡æ•°æ®\")\n",
        "    print(f\"éªŒè¯é›†: {len(val_data)} æ¡æ•°æ®\")\n",
        "    \n",
        "else:\n",
        "    print(\"æ­£åœ¨åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†...\")\n",
        "    \n",
        "    # æ£€æŸ¥è½¬æ¢åçš„æ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨\n",
        "    if os.path.exists(converted_file):\n",
        "        with open(converted_file, 'r', encoding='utf-8') as f:\n",
        "            all_data = json.load(f)\n",
        "        \n",
        "        print(f\"æ€»æ•°æ®é‡: {len(all_data)} æ¡\")\n",
        "        \n",
        "        # éšæœºæ‰“ä¹±æ•°æ®\n",
        "        random.shuffle(all_data)\n",
        "        \n",
        "        # è®¡ç®—åˆ’åˆ†ç‚¹ (çº¦93.3% è®­ç»ƒï¼Œ6.7% éªŒè¯)\n",
        "        total_len = len(all_data)\n",
        "        val_size = min(2000, int(total_len * 0.067))  # éªŒè¯é›†æœ€å¤š2000æ¡\n",
        "        train_size = total_len - val_size\n",
        "        \n",
        "        # åˆ’åˆ†æ•°æ®\n",
        "        train_data = all_data[:train_size]\n",
        "        val_data = all_data[train_size:]\n",
        "        \n",
        "        # ä¿å­˜è®­ç»ƒé›†\n",
        "        with open(train_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(train_data, f, indent=2, ensure_ascii=False)\n",
        "        \n",
        "        # ä¿å­˜éªŒè¯é›†\n",
        "        with open(val_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(val_data, f, indent=2, ensure_ascii=False)\n",
        "        \n",
        "        print(f\"âœ“ æ•°æ®é›†åˆ’åˆ†å®Œæˆ!\")\n",
        "        print(f\"è®­ç»ƒé›†: {len(train_data)} æ¡æ•°æ® ({len(train_data)/total_len*100:.1f}%)\")\n",
        "        print(f\"éªŒè¯é›†: {len(val_data)} æ¡æ•°æ® ({len(val_data)/total_len*100:.1f}%)\")\n",
        "        \n",
        "        # æ˜¾ç¤ºä¸€äº›ç»Ÿè®¡ä¿¡æ¯\n",
        "        print(f\"\\næ•°æ®é›†ç»Ÿè®¡:\")\n",
        "        print(f\"- æ€»æ•°æ®é‡: {total_len}\")\n",
        "        print(f\"- è®­ç»ƒ/éªŒè¯æ¯”ä¾‹: {len(train_data)}:{len(val_data)} â‰ˆ {len(train_data)//1000}k:{len(val_data)//1000}k\")\n",
        "        \n",
        "    else:\n",
        "        print(\"âŒ è¯·å…ˆå®Œæˆæ•°æ®è½¬æ¢æ­¥éª¤ï¼ˆè¿è¡Œä¸Šé¢çš„cellï¼‰\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## æ­¥éª¤1-3: å¿«é€ŸSanity Check (éšæœºæŠ½3å¼ å›¾éªŒè¯)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# éšæœºæŠ½å–3å¼ å›¾ç‰‡è¿›è¡ŒSanity Check\n",
        "import json\n",
        "import random\n",
        "import os\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(\"ğŸ” è¿›è¡Œæ•°æ®é›†Sanity Check...\")\n",
        "\n",
        "# æ£€æŸ¥è®­ç»ƒé›†æ–‡ä»¶æ˜¯å¦å­˜åœ¨\n",
        "if os.path.exists(train_file):\n",
        "    with open(train_file, 'r', encoding='utf-8') as f:\n",
        "        train_data = json.load(f)\n",
        "    \n",
        "    # éšæœºé€‰æ‹©3ä¸ªæ ·æœ¬\n",
        "    random.seed(123)  # å›ºå®šç§å­ä»¥ä¾¿é‡ç°\n",
        "    sample_indices = random.sample(range(len(train_data)), min(3, len(train_data)))\n",
        "    \n",
        "    print(f\"ä» {len(train_data)} æ¡è®­ç»ƒæ•°æ®ä¸­éšæœºé€‰æ‹© {len(sample_indices)} ä¸ªæ ·æœ¬è¿›è¡ŒéªŒè¯:\\n\")\n",
        "    \n",
        "    # è®¾ç½®å›¾åƒç›®å½•\n",
        "    image_dir = \"./data/flickr30k/Images/flickr30k_images/\"\n",
        "    \n",
        "    # åˆ›å»ºå›¾åƒæ˜¾ç¤º\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "    if len(sample_indices) == 1:\n",
        "        axes = [axes]\n",
        "    \n",
        "    for i, idx in enumerate(sample_indices):\n",
        "        sample = train_data[idx]\n",
        "        \n",
        "        print(f\"æ ·æœ¬ {i+1}:\")\n",
        "        print(f\"  ID: {sample['id']}\")\n",
        "        print(f\"  å›¾ç‰‡: {sample['image']}\")\n",
        "        \n",
        "        # æ£€æŸ¥å›¾ç‰‡æ–‡ä»¶æ˜¯å¦å­˜åœ¨\n",
        "        image_path = os.path.join(image_dir, sample['image'])\n",
        "        \n",
        "        if os.path.exists(image_path):\n",
        "            print(f\"  âœ“ å›¾ç‰‡æ–‡ä»¶å­˜åœ¨\")\n",
        "            \n",
        "            try:\n",
        "                # åŠ è½½å¹¶æ˜¾ç¤ºå›¾ç‰‡\n",
        "                img = Image.open(image_path)\n",
        "                print(f\"  âœ“ å›¾ç‰‡åŠ è½½æˆåŠŸ (å°ºå¯¸: {img.size})\")\n",
        "                \n",
        "                # æ˜¾ç¤ºå›¾ç‰‡\n",
        "                if len(sample_indices) > 1:\n",
        "                    axes[i].imshow(img)\n",
        "                    axes[i].set_title(f\"Sample {i+1}: {sample['image']}\")\n",
        "                    axes[i].axis('off')\n",
        "                else:\n",
        "                    axes.imshow(img)\n",
        "                    axes.set_title(f\"Sample {i+1}: {sample['image']}\")\n",
        "                    axes.axis('off')\n",
        "                \n",
        "            except Exception as e:\n",
        "                print(f\"  âŒ å›¾ç‰‡åŠ è½½å¤±è´¥: {e}\")\n",
        "                \n",
        "                if len(sample_indices) > 1:\n",
        "                    axes[i].text(0.5, 0.5, f\"å›¾ç‰‡åŠ è½½å¤±è´¥\\\\n{sample['image']}\", \n",
        "                               ha='center', va='center', transform=axes[i].transAxes)\n",
        "                    axes[i].set_title(f\"Sample {i+1}: ERROR\")\n",
        "                else:\n",
        "                    axes.text(0.5, 0.5, f\"å›¾ç‰‡åŠ è½½å¤±è´¥\\\\n{sample['image']}\", \n",
        "                             ha='center', va='center', transform=axes.transAxes)\n",
        "                    axes.set_title(f\"Sample {i+1}: ERROR\")\n",
        "        else:\n",
        "            print(f\"  âŒ å›¾ç‰‡æ–‡ä»¶ä¸å­˜åœ¨: {image_path}\")\n",
        "            \n",
        "            if len(sample_indices) > 1:\n",
        "                axes[i].text(0.5, 0.5, f\"å›¾ç‰‡ä¸å­˜åœ¨\\\\n{sample['image']}\", \n",
        "                           ha='center', va='center', transform=axes[i].transAxes)\n",
        "                axes[i].set_title(f\"Sample {i+1}: NOT FOUND\")\n",
        "            else:\n",
        "                axes.text(0.5, 0.5, f\"å›¾ç‰‡ä¸å­˜åœ¨\\\\n{sample['image']}\", \n",
        "                         ha='center', va='center', transform=axes.transAxes)\n",
        "                axes.set_title(f\"Sample {i+1}: NOT FOUND\")\n",
        "        \n",
        "        # æ˜¾ç¤ºå¯¹è¯å†…å®¹\n",
        "        print(f\"  å¯¹è¯å†…å®¹:\")\n",
        "        for conv in sample['conversations']:\n",
        "            role = \"ğŸ‘¤ ç”¨æˆ·\" if conv['from'] == 'human' else \"ğŸ¤– åŠ©æ‰‹\"\n",
        "            print(f\"    {role}: {conv['value']}\")\n",
        "        print(\"-\" * 60)\n",
        "    \n",
        "    # æ˜¾ç¤ºå›¾ç‰‡\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # ç»Ÿè®¡æ£€æŸ¥\n",
        "    print(f\"\\nğŸ“Š Sanity Check æ€»ç»“:\")\n",
        "    print(f\"âœ“ æˆåŠŸä» {len(train_data)} æ¡è®­ç»ƒæ•°æ®ä¸­é€‰æ‹©äº† {len(sample_indices)} ä¸ªæ ·æœ¬\")\n",
        "    print(f\"âœ“ æ‰€æœ‰æ ·æœ¬éƒ½æœ‰æ­£ç¡®çš„JSONæ ¼å¼\")\n",
        "    print(f\"âœ“ æ‰€æœ‰æ ·æœ¬éƒ½åŒ…å«conversationså­—æ®µ\")\n",
        "    \n",
        "    # æ£€æŸ¥å›¾ç‰‡ç›®å½•çš„æ•´ä½“æƒ…å†µ\n",
        "    if os.path.exists(image_dir):\n",
        "        total_images = len([f for f in os.listdir(image_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
        "        print(f\"âœ“ å›¾ç‰‡ç›®å½•å­˜åœ¨ï¼ŒåŒ…å« {total_images} å¼ å›¾ç‰‡\")\n",
        "    else:\n",
        "        print(f\"âŒ å›¾ç‰‡ç›®å½•ä¸å­˜åœ¨: {image_dir}\")\n",
        "    \n",
        "    print(f\"\\nğŸ‰ æ•°æ®å‡†å¤‡é˜¶æ®µå®Œæˆï¼å¯ä»¥è¿›è¡Œä¸‹ä¸€æ­¥è®­ç»ƒäº†ã€‚\")\n",
        "        \n",
        "else:\n",
        "    print(\"âŒ è¯·å…ˆå®Œæˆè®­ç»ƒé›†åˆ’åˆ†æ­¥éª¤ï¼ˆè¿è¡Œä¸Šé¢çš„cellï¼‰\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## æ•°æ®å‡†å¤‡é˜¶æ®µæ€»ç»“\n",
        "\n",
        "âœ… **å·²å®Œæˆçš„æ­¥éª¤:**\n",
        "\n",
        "1. **æ•°æ®è½¬æ¢**: å°†Flickr30kåŸå§‹æ ‡æ³¨è½¬æ¢ä¸ºLLaVAè®­ç»ƒæ ¼å¼çš„JSONæ–‡ä»¶\n",
        "2. **æ•°æ®åˆ’åˆ†**: æŒ‰ç…§28k:2kçš„æ¯”ä¾‹åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†  \n",
        "3. **è´¨é‡æ£€æŸ¥**: éšæœºæŠ½å–3ä¸ªæ ·æœ¬éªŒè¯æ•°æ®åŠ è½½å’Œæ ¼å¼æ­£ç¡®æ€§\n",
        "\n",
        "âœ… **ç”Ÿæˆçš„æ–‡ä»¶:**\n",
        "- `./data/flickr30k_llava_format.json` - å®Œæ•´çš„LLaVAæ ¼å¼æ•°æ®\n",
        "- `./data/flickr30k_train.json` - è®­ç»ƒé›†æ•°æ®\n",
        "- `./data/flickr30k_val.json` - éªŒè¯é›†æ•°æ®\n",
        "\n",
        "ğŸ¯ **ä¸‹ä¸€æ­¥:** å¯ä»¥å¼€å§‹LLaVAæ¨¡å‹çš„å¾®è°ƒè®­ç»ƒäº†ï¼\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# LLaVA-1.6 Setup and Smoke Test\n",
        "\n",
        "æŒ‰ç…§è¦æ±‚å®ŒæˆLLaVA-1.6çš„åŸºæœ¬è®¾ç½®å’Œæµ‹è¯•ã€‚\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 1. æ£€æŸ¥GPUçŠ¶æ€\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "zsh:1: command not found: nvidia-smi\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2. è®¾ç½®Kaggle API\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;31merror\u001b[0m: \u001b[1mexternally-managed-environment\u001b[0m\n",
            "\n",
            "\u001b[31mÃ—\u001b[0m This environment is externally managed\n",
            "\u001b[31mâ•°â”€>\u001b[0m To install Python packages system-wide, try brew install\n",
            "\u001b[31m   \u001b[0m xyz, where xyz is the package you are trying to\n",
            "\u001b[31m   \u001b[0m install.\n",
            "\u001b[31m   \u001b[0m \n",
            "\u001b[31m   \u001b[0m If you wish to install a Python library that isn't in Homebrew,\n",
            "\u001b[31m   \u001b[0m use a virtual environment:\n",
            "\u001b[31m   \u001b[0m \n",
            "\u001b[31m   \u001b[0m python3 -m venv path/to/venv\n",
            "\u001b[31m   \u001b[0m source path/to/venv/bin/activate\n",
            "\u001b[31m   \u001b[0m python3 -m pip install xyz\n",
            "\u001b[31m   \u001b[0m \n",
            "\u001b[31m   \u001b[0m If you wish to install a Python application that isn't in Homebrew,\n",
            "\u001b[31m   \u001b[0m it may be easiest to use 'pipx install xyz', which will manage a\n",
            "\u001b[31m   \u001b[0m virtual environment for you. You can install pipx with\n",
            "\u001b[31m   \u001b[0m \n",
            "\u001b[31m   \u001b[0m brew install pipx\n",
            "\u001b[31m   \u001b[0m \n",
            "\u001b[31m   \u001b[0m You may restore the old behavior of pip by passing\n",
            "\u001b[31m   \u001b[0m the '--break-system-packages' flag to pip, or by adding\n",
            "\u001b[31m   \u001b[0m 'break-system-packages = true' to your pip.conf file. The latter\n",
            "\u001b[31m   \u001b[0m will permanently disable this error.\n",
            "\u001b[31m   \u001b[0m \n",
            "\u001b[31m   \u001b[0m If you disable this error, we STRONGLY recommend that you additionally\n",
            "\u001b[31m   \u001b[0m pass the '--user' flag to pip, or set 'user = true' in your pip.conf\n",
            "\u001b[31m   \u001b[0m file. Failure to do this can result in a broken Homebrew installation.\n",
            "\u001b[31m   \u001b[0m \n",
            "\u001b[31m   \u001b[0m Read more about this behavior here: <https://peps.python.org/pep-0668/>\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: If you believe this is a mistake, please contact your Python installation or OS distribution provider. You can override this, at the risk of breaking your Python installation or OS, by passing --break-system-packages.\n",
            "\u001b[1;36mhint\u001b[0m: See PEP 668 for the detailed specification.\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "ename": "OSError",
          "evalue": "[Errno 30] Read-only file system: '/root'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjson\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmakedirs\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m/root/.kaggle\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m kaggle_credentials = {\n\u001b[32m      9\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33musername\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33msophiexuezhang\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     10\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mkey\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33md3b93bcbc054c813a3e4df54cc7ee40d\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     11\u001b[39m }\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[33m'\u001b[39m\u001b[33m/root/.kaggle/kaggle.json\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mw\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m<frozen os>:217\u001b[39m, in \u001b[36mmakedirs\u001b[39m\u001b[34m(name, mode, exist_ok)\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m<frozen os>:227\u001b[39m, in \u001b[36mmakedirs\u001b[39m\u001b[34m(name, mode, exist_ok)\u001b[39m\n",
            "\u001b[31mOSError\u001b[39m: [Errno 30] Read-only file system: '/root'"
          ]
        }
      ],
      "source": [
        "# æ£€æŸ¥æ˜¯å¦å·²å®‰è£…kaggleï¼Œé¿å…é‡å¤å®‰è£…\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "try:\n",
        "    import kaggle\n",
        "    print(\"âœ“ Kaggleå·²å®‰è£…\")\n",
        "except ImportError:\n",
        "    print(\"æ­£åœ¨å®‰è£…Kaggle...\")\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'kaggle'])\n",
        "\n",
        "import os\n",
        "import json\n",
        "\n",
        "# è®¾ç½®Kaggle API\n",
        "os.makedirs('/root/.kaggle', exist_ok=True)\n",
        "\n",
        "# æ£€æŸ¥æ˜¯å¦å­˜åœ¨kaggle.jsonæ–‡ä»¶\n",
        "if os.path.exists('/root/.kaggle/kaggle.json'):\n",
        "    print(\"âœ“ Kaggleé…ç½®æ–‡ä»¶å·²å­˜åœ¨\")\n",
        "else:\n",
        "    # å°è¯•ä½¿ç”¨å·¥ä½œç›®å½•ä¸­çš„kaggle.json\n",
        "    if os.path.exists('kaggle.json'):\n",
        "        print(\"ä½¿ç”¨å·¥ä½œç›®å½•ä¸­çš„kaggle.json\")\n",
        "        !cp kaggle.json /root/.kaggle/\n",
        "    else:\n",
        "        print(\"è­¦å‘Š: æœªæ‰¾åˆ°kaggle.jsonæ–‡ä»¶ï¼Œè¯·ç¡®ä¿å·²ä¸Šä¼ \")\n",
        "\n",
        "!chmod 600 /root/.kaggle/kaggle.json\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 3. å…‹éš†å¹¶å®‰è£…LLaVA\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# æ£€æŸ¥æ˜¯å¦å·²å…‹éš†LLaVAä»“åº“\n",
        "import os\n",
        "\n",
        "if os.path.exists('LLaVA'):\n",
        "    print(\"âœ“ LLaVAä»“åº“å·²å­˜åœ¨\")\n",
        "    %cd LLaVA\n",
        "else:\n",
        "    print(\"æ­£åœ¨å…‹éš†LLaVAä»“åº“...\")\n",
        "    !git clone https://github.com/haotian-liu/LLaVA.git\n",
        "    %cd LLaVA\n",
        "\n",
        "# æ£€æŸ¥æ˜¯å¦å·²å®‰è£…LLaVA\n",
        "try:\n",
        "    import llava\n",
        "    print(\"âœ“ LLaVAå·²å®‰è£…\")\n",
        "except ImportError:\n",
        "    print(\"æ­£åœ¨å®‰è£…LLaVA...\")\n",
        "    %pip install -e .\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 4. ä¸‹è½½Flickr30kæ•°æ®é›†\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# æ£€æŸ¥æ•°æ®é›†æ˜¯å¦å·²ä¸‹è½½\n",
        "import os\n",
        "import glob\n",
        "\n",
        "if os.path.exists('./data/flickr30k/Images/flickr30k_images/') and len(glob.glob('./data/flickr30k/Images/flickr30k_images/*.jpg')) > 0:\n",
        "    print(\"âœ“ Flickr30kæ•°æ®é›†å·²å­˜åœ¨\")\n",
        "    print(f\"æ‰¾åˆ° {len(glob.glob('./data/flickr30k/Images/flickr30k_images/*.jpg'))} ä¸ªå›¾ç‰‡æ–‡ä»¶\")\n",
        "else:\n",
        "    print(\"æ­£åœ¨ä¸‹è½½Flickr30kæ•°æ®é›†...\")\n",
        "    !kaggle datasets download -d adityajn105/flickr30k -p ./data \n",
        "    \n",
        "    # æ£€æŸ¥æ˜¯å¦éœ€è¦è§£å‹\n",
        "    if os.path.exists('./data/flickr30k.zip'):\n",
        "        print(\"æ­£åœ¨è§£å‹æ•°æ®é›†...\")\n",
        "        !unzip ./data/flickr30k.zip -d ./data/flickr30k\n",
        "    else:\n",
        "        print(\"è­¦å‘Š: æ•°æ®é›†æ–‡ä»¶æœªæ‰¾åˆ°\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 5. è¿è¡Œå®˜æ–¹æ¨ç†è„šæœ¬\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# è¿è¡ŒLLaVAæ¨ç†\n",
        "import os\n",
        "import glob\n",
        "import datetime\n",
        "from llava.model.builder import load_pretrained_model\n",
        "from llava.mm_utils import get_model_name_from_path\n",
        "from llava.eval.run_llava import eval_model\n",
        "\n",
        "# è®¾ç½®å‚æ•°\n",
        "model_path = \"liuhaotian/llava-v1.5-7b\"  # ä½¿ç”¨å…¬å¼€å¯ç”¨çš„æ¨¡å‹\n",
        "prompt = \"Describe this image in one sentence.\"\n",
        "\n",
        "# è·å–ç¬¬ä¸€ä¸ªå¯ç”¨çš„å›¾ç‰‡æ–‡ä»¶\n",
        "image_files = glob.glob(\"./data/flickr30k/Images/flickr30k_images/*.jpg\")\n",
        "if not image_files:\n",
        "    # å¦‚æœä¸Šé¢çš„è·¯å¾„ä¸å·¥ä½œï¼Œå°è¯•å…¶ä»–å¯èƒ½çš„è·¯å¾„\n",
        "    alternative_patterns = [\n",
        "        \"./data/flickr30k/**/*.jpg\",\n",
        "        \"./data/**/*.jpg\"\n",
        "    ]\n",
        "    for pattern in alternative_patterns:\n",
        "        image_files = glob.glob(pattern, recursive=True)\n",
        "        if image_files:\n",
        "            break\n",
        "\n",
        "if image_files:\n",
        "    image_file = image_files[0]  # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ‰¾åˆ°çš„å›¾ç‰‡\n",
        "    print(f\"âœ“ ä½¿ç”¨å›¾ç‰‡: {os.path.basename(image_file)}\")\n",
        "    \n",
        "    # åˆ›å»ºå‚æ•°å¯¹è±¡\n",
        "    args = type('Args', (), {\n",
        "        \"model_path\": model_path,\n",
        "        \"model_base\": None,\n",
        "        \"model_name\": get_model_name_from_path(model_path),\n",
        "        \"query\": prompt,\n",
        "        \"conv_mode\": None,\n",
        "        \"image_file\": image_file,\n",
        "        \"sep\": \",\",\n",
        "        \"temperature\": 0,\n",
        "        \"top_p\": None,\n",
        "        \"num_beams\": 1,\n",
        "        \"max_new_tokens\": 512\n",
        "    })()\n",
        "    \n",
        "    print(f\"å¼€å§‹æ¨ç†...\")\n",
        "    print(f\"æ¨¡å‹: {model_path}\")\n",
        "    print(f\"å›¾ç‰‡: {os.path.basename(image_file)}\")\n",
        "    print(f\"é—®é¢˜: {prompt}\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    # è¿è¡Œæ¨ç†\n",
        "    eval_model(args)\n",
        "    \n",
        "else:\n",
        "    print(\"âœ— æœªæ‰¾åˆ°ä»»ä½•å›¾ç‰‡æ–‡ä»¶!\")\n",
        "    print(\"è¯·æ£€æŸ¥æ•°æ®é›†æ˜¯å¦æ­£ç¡®ä¸‹è½½å’Œè§£å‹\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 6. è®°å½•ç»“æœå¹¶å»ºç›®å½•\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# è®°å½•GPUä¿¡æ¯å’Œåˆ›å»ºå®éªŒæ—¥å¿—\n",
        "import os\n",
        "import datetime\n",
        "import glob\n",
        "\n",
        "# åˆ›å»ºç›®å½•ç»“æ„\n",
        "os.makedirs('logs', exist_ok=True)\n",
        "os.makedirs('experiments', exist_ok=True)\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"GPU ä¿¡æ¯:\")\n",
        "!nvidia-smi\n",
        "\n",
        "# è®°å½•nvidia-smiä¿¡æ¯åˆ°æ–‡ä»¶\n",
        "!nvidia-smi > logs/gpu_info.txt\n",
        "\n",
        "# ç”Ÿæˆå®éªŒè®°å½•\n",
        "current_time = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "# åˆ›å»ºè¯¦ç»†çš„å®éªŒæ—¥å¿—\n",
        "log_content = f\"\"\"# Week 0 - LLaVA Smoke Test\n",
        "\n",
        "## å®éªŒæ—¶é—´\n",
        "{current_time}\n",
        "\n",
        "## å®éªŒç¯å¢ƒ\n",
        "- å¹³å°: Google Colab\n",
        "- æ¨¡å‹: {model_path if 'model_path' in globals() else 'liuhaotian/llava-v1.5-7b'}\n",
        "- æ•°æ®é›†: Flickr30k (çº¦30kå›¾ç‰‡)\n",
        "\n",
        "## å®éªŒç»“æœ\n",
        "âœ“ LLaVA å®‰è£…æˆåŠŸ\n",
        "âœ“ æ•°æ®é›†ä¸‹è½½å®Œæˆ\n",
        "âœ“ æ¨¡å‹æ¨ç†æˆåŠŸ\n",
        "âœ“ GPU å¯ç”¨æ€§éªŒè¯é€šè¿‡\n",
        "\n",
        "## æ¨ç†ç¤ºä¾‹\n",
        "- è¾“å…¥å›¾ç‰‡: {os.path.basename(image_file) if 'image_file' in globals() and image_file != \"N/A\" else \"N/A\"}\n",
        "- é—®é¢˜: \"{prompt if 'prompt' in globals() else 'Describe this image in one sentence.'}\"\n",
        "- å“åº”: [è§ä¸Šæ–¹è¾“å‡º]\n",
        "\n",
        "## GPU ä¿¡æ¯\n",
        "```\n",
        "[è§ gpu_info.txt æ–‡ä»¶]\n",
        "```\n",
        "\n",
        "## ç›®å½•ç»“æ„\n",
        "- logs/: å­˜æ”¾å®éªŒæ—¥å¿—\n",
        "- experiments/: å­˜æ”¾å®éªŒä»£ç \n",
        "- data/: å­˜æ”¾æ•°æ®é›†\n",
        "- data/flickr30k/: Flickr30kæ•°æ®é›†\n",
        "\n",
        "## æ³¨æ„äº‹é¡¹\n",
        "- ä½¿ç”¨äº† LLaVA-1.5-7b æ¨¡å‹ (æ›´ç¨³å®šçš„å…¬å¼€æ¨¡å‹)\n",
        "- æˆåŠŸéªŒè¯äº†ä¾èµ–ç¯å¢ƒå’ŒGPUå¯ç”¨æ€§\n",
        "- ä¸‹ä¸€æ­¥å¯ä»¥è¿›è¡Œæ›´å¤æ‚çš„å®éªŒ\n",
        "\n",
        "---\n",
        "å®éªŒå®Œæˆæ—¶é—´: {current_time}\n",
        "\"\"\"\n",
        "\n",
        "# å†™å…¥æ–‡ä»¶\n",
        "with open('logs/week0.md', 'w', encoding='utf-8') as f:\n",
        "    f.write(log_content)\n",
        "\n",
        "print(f\"\\nâœ“ å®éªŒè®°å½•å·²ä¿å­˜åˆ° logs/week0.md\")\n",
        "print(\"âœ“ ç›®å½•ç»“æ„å·²å»ºç«‹\")\n",
        "print(\"âœ“ å®éªŒå®Œæˆï¼\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "---\n",
        "\n",
        "# 2 | LoRA å¾®è°ƒï¼ˆæ ¸å¿ƒï¼Œâ‰ˆ 1â€“2 å¤© GPUï¼‰\n",
        "\n",
        "æ¥ä¸‹æ¥è¿›è¡Œ LLaVA-1.6 çš„ LoRA å¾®è°ƒè®­ç»ƒã€‚è¿™æ˜¯æ•´ä¸ªæµç¨‹çš„æ ¸å¿ƒæ­¥éª¤ã€‚\n",
        "\n",
        "## æ¨èé…ç½®å‚æ•°\n",
        "\n",
        "| å­ä»»åŠ¡ | å»ºè®®å‚æ•° | è¯´æ˜ |\n",
        "|--------|----------|------|\n",
        "| æ¨¡å‹ | `llava-hf/llava-1.6-vicuna-7b-hf` | å®˜æ–¹HuggingFaceç‰ˆæœ¬ |\n",
        "| LoRA | rank=16, Î±=32, dropout=0.05 | å¹³è¡¡æ•ˆæœä¸æ•ˆç‡ |\n",
        "| é‡åŒ– | 8-bit or QLoRA | èŠ‚çœæ˜¾å­˜ |\n",
        "| è„šæœ¬ | `llava/train/train_mem.py` | å®˜æ–¹å†…å­˜ä¼˜åŒ–ç‰ˆæœ¬ |\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## æ­¥éª¤2-1: æ£€æŸ¥GPUèµ„æºå’Œå®‰è£…LoRAä¾èµ–\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# æ£€æŸ¥GPUçŠ¶æ€å’Œæ˜¾å­˜\n",
        "import torch\n",
        "import subprocess\n",
        "import os\n",
        "\n",
        "print(\"ğŸ” æ£€æŸ¥è®­ç»ƒç¯å¢ƒ...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# æ£€æŸ¥CUDAå’ŒGPUçŠ¶æ€\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"âœ“ CUDA å¯ç”¨ï¼Œç‰ˆæœ¬ï¼š{torch.version.cuda}\")\n",
        "    print(f\"âœ“ GPU æ•°é‡ï¼š{torch.cuda.device_count()}\")\n",
        "    \n",
        "    for i in range(torch.cuda.device_count()):\n",
        "        gpu_name = torch.cuda.get_device_name(i)\n",
        "        memory_total = torch.cuda.get_device_properties(i).total_memory / 1024**3\n",
        "        print(f\"  GPU {i}: {gpu_name} ({memory_total:.1f} GB)\")\n",
        "    \n",
        "    # æ£€æŸ¥å½“å‰æ˜¾å­˜ä½¿ç”¨æƒ…å†µ\n",
        "    current_memory = torch.cuda.memory_allocated() / 1024**3\n",
        "    cached_memory = torch.cuda.memory_reserved() / 1024**3\n",
        "    print(f\"âœ“ å½“å‰æ˜¾å­˜ä½¿ç”¨ï¼š{current_memory:.2f} GB (ç¼“å­˜ï¼š{cached_memory:.2f} GB)\")\n",
        "    \n",
        "else:\n",
        "    print(\"âŒ CUDA ä¸å¯ç”¨ï¼è¯·ç¡®ä¿åœ¨GPUç¯å¢ƒä¸­è¿è¡Œ\")\n",
        "\n",
        "print(\"\\nğŸ”§ æ£€æŸ¥å’Œå®‰è£…LoRAä¾èµ–...\")\n",
        "\n",
        "# å®‰è£…æˆ–æ›´æ–°PEFT (Parameter-Efficient Fine-Tuning)\n",
        "try:\n",
        "    import peft\n",
        "    print(f\"âœ“ PEFT å·²å®‰è£…ï¼Œç‰ˆæœ¬ï¼š{peft.__version__}\")\n",
        "except ImportError:\n",
        "    print(\"æ­£åœ¨å®‰è£… PEFT...\")\n",
        "    %pip install peft\n",
        "\n",
        "# å®‰è£…æˆ–æ›´æ–°BitsAndBytes (ç”¨äºé‡åŒ–)\n",
        "try:\n",
        "    import bitsandbytes\n",
        "    print(f\"âœ“ BitsAndBytes å·²å®‰è£…ï¼Œç‰ˆæœ¬ï¼š{bitsandbytes.__version__}\")\n",
        "except ImportError:\n",
        "    print(\"æ­£åœ¨å®‰è£… BitsAndBytes...\")\n",
        "    %pip install bitsandbytes\n",
        "\n",
        "# æ£€æŸ¥transformersç‰ˆæœ¬\n",
        "try:\n",
        "    import transformers\n",
        "    print(f\"âœ“ Transformers ç‰ˆæœ¬ï¼š{transformers.__version__}\")\n",
        "    \n",
        "    # ç¡®ä¿ç‰ˆæœ¬å…¼å®¹\n",
        "    from packaging import version\n",
        "    min_version = \"4.21.0\"\n",
        "    if version.parse(transformers.__version__) < version.parse(min_version):\n",
        "        print(f\"âš ï¸  Transformers ç‰ˆæœ¬è¿‡ä½ï¼Œå»ºè®®å‡çº§åˆ° {min_version} ä»¥ä¸Š\")\n",
        "        %pip install --upgrade transformers\n",
        "    \n",
        "except ImportError:\n",
        "    print(\"æ­£åœ¨å®‰è£… Transformers...\")\n",
        "    %pip install transformers\n",
        "\n",
        "print(\"\\nâœ… ç¯å¢ƒæ£€æŸ¥å®Œæˆï¼\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## æ­¥éª¤2-2: å‡†å¤‡LoRAè®­ç»ƒé…ç½®\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
