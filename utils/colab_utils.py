#!/usr/bin/env python3
"""
Colabé›†æˆå·¥å…·å‡½æ•°
æä¾›æœ¬åœ°å¼€å‘ä¸Colab GPUç¯å¢ƒä¹‹é—´çš„æ¡¥æ¥åŠŸèƒ½
"""

import os
import json
import shutil
import subprocess
import zipfile
from pathlib import Path
import requests
from typing import Dict, List, Optional

class ColabSync:
    """Colabä»£ç åŒæ­¥å·¥å…·"""
    
    def __init__(self, config_path: str = 'configs/colab_config.json'):
        self.config = self.load_config(config_path)
        self.local_path = self.config.get('sync', {}).get('local_path', '/content/ai-ml-pipeline')
        
    def load_config(self, config_path: str) -> Dict:
        """åŠ è½½Colabé…ç½®"""
        if os.path.exists(config_path):
            with open(config_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        return {}
    
    def setup_colab_environment(self):
        """è®¾ç½®Colabç¯å¢ƒ"""
        print("ğŸš€ è®¾ç½®Colabç¯å¢ƒ...")
        
        # æ£€æŸ¥GPU
        try:
            import torch
            if torch.cuda.is_available():
                print(f"âœ… GPUå¯ç”¨: {torch.cuda.get_device_name(0)}")
                print(f"GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
            else:
                print("âš ï¸ GPUä¸å¯ç”¨")
        except ImportError:
            print("âš ï¸ PyTorchæœªå®‰è£…")
        
        # æŒ‚è½½Google Drive
        try:
            from google.colab import drive
            drive.mount('/content/drive')
            print("âœ… Google Driveå·²æŒ‚è½½")
        except ImportError:
            print("âš ï¸ ä¸åœ¨Colabç¯å¢ƒä¸­")
        
        # åˆ›å»ºå·¥ä½œç›®å½•
        os.makedirs(self.local_path, exist_ok=True)
        os.chdir(self.local_path)
        print(f"âœ… å·¥ä½œç›®å½•: {os.getcwd()}")
    
    def sync_from_github(self, repo_url: str, branch: str = 'main'):
        """ä»GitHubåŒæ­¥ä»£ç """
        print(f"ğŸ“¥ ä»GitHubåŒæ­¥ä»£ç : {repo_url}")
        
        if os.path.exists('.git'):
            # æ›´æ–°ç°æœ‰ä»“åº“
            result = subprocess.run(['git', 'pull', 'origin', branch], 
                                  capture_output=True, text=True)
            if result.returncode == 0:
                print("âœ… ä»£ç å·²æ›´æ–°")
            else:
                print(f"âš ï¸ æ›´æ–°å¤±è´¥: {result.stderr}")
        else:
            # å…‹éš†æ–°ä»“åº“
            result = subprocess.run(['git', 'clone', repo_url, '.'], 
                                  capture_output=True, text=True)
            if result.returncode == 0:
                print("âœ… ä»£ç å·²å…‹éš†")
            else:
                print(f"âš ï¸ å…‹éš†å¤±è´¥: {result.stderr}")
    
    def upload_from_local(self, zip_path: str):
        """ä»æœ¬åœ°ä¸Šä¼ zipæ–‡ä»¶"""
        print(f"ğŸ“ è§£å‹æ–‡ä»¶: {zip_path}")
        
        if not os.path.exists(zip_path):
            print(f"âš ï¸ æ–‡ä»¶ä¸å­˜åœ¨: {zip_path}")
            return
        
        with zipfile.ZipFile(zip_path, 'r') as zip_ref:
            zip_ref.extractall('.')
        
        print("âœ… æ–‡ä»¶å·²è§£å‹")
        os.remove(zip_path)  # åˆ é™¤zipæ–‡ä»¶
    
    def install_requirements(self, requirements_path: str = 'requirements.txt'):
        """å®‰è£…é¡¹ç›®ä¾èµ–"""
        print("ğŸ“¦ å®‰è£…é¡¹ç›®ä¾èµ–...")
        
        if os.path.exists(requirements_path):
            result = subprocess.run(['pip', 'install', '-r', requirements_path], 
                                  capture_output=True, text=True)
            if result.returncode == 0:
                print("âœ… ä¾èµ–å®‰è£…å®Œæˆ")
            else:
                print(f"âš ï¸ å®‰è£…å¤±è´¥: {result.stderr}")
        else:
            # å®‰è£…åŸºç¡€ä¾èµ–
            basic_packages = [
                'torch', 'torchvision', 'torchaudio',
                'transformers', 'datasets', 'wandb',
                'matplotlib', 'seaborn', 'tqdm'
            ]
            for package in basic_packages:
                subprocess.run(['pip', 'install', package], 
                             capture_output=True, text=True)
            print("âœ… åŸºç¡€ä¾èµ–å®‰è£…å®Œæˆ")
    
    def setup_python_path(self):
        """è®¾ç½®Pythonè·¯å¾„"""
        import sys
        paths_to_add = [
            os.path.join(self.local_path, 'src'),
            os.path.join(self.local_path, 'utils')
        ]
        
        for path in paths_to_add:
            if path not in sys.path:
                sys.path.append(path)
        
        print("âœ… Pythonè·¯å¾„å·²è®¾ç½®")
    
    def save_to_drive(self, source_dir: str, drive_path: str = '/content/drive/MyDrive/ai-ml-results'):
        """ä¿å­˜ç»“æœåˆ°Google Drive"""
        print(f"ğŸ’¾ ä¿å­˜åˆ°Drive: {drive_path}")
        
        os.makedirs(drive_path, exist_ok=True)
        
        if os.path.exists(source_dir):
            if os.path.isfile(source_dir):
                shutil.copy2(source_dir, drive_path)
            else:
                dest_dir = os.path.join(drive_path, os.path.basename(source_dir))
                shutil.copytree(source_dir, dest_dir, dirs_exist_ok=True)
            print(f"âœ… å·²ä¿å­˜: {source_dir} -> {drive_path}")
        else:
            print(f"âš ï¸ æºç›®å½•ä¸å­˜åœ¨: {source_dir}")

class ColabMonitor:
    """Colabèµ„æºç›‘æ§å·¥å…·"""
    
    @staticmethod
    def check_gpu_memory():
        """æ£€æŸ¥GPUå†…å­˜ä½¿ç”¨æƒ…å†µ"""
        try:
            import torch
            if torch.cuda.is_available():
                allocated = torch.cuda.memory_allocated() / 1024**3
                reserved = torch.cuda.memory_reserved() / 1024**3
                total = torch.cuda.get_device_properties(0).total_memory / 1024**3
                
                print(f"GPUå†…å­˜ä½¿ç”¨æƒ…å†µ:")
                print(f"  å·²åˆ†é…: {allocated:.2f} GB")
                print(f"  å·²ä¿ç•™: {reserved:.2f} GB") 
                print(f"  æ€»å†…å­˜: {total:.2f} GB")
                print(f"  å‰©ä½™: {total - reserved:.2f} GB")
                
                return {
                    'allocated': allocated,
                    'reserved': reserved,
                    'total': total,
                    'free': total - reserved
                }
            else:
                print("âš ï¸ GPUä¸å¯ç”¨")
                return None
        except ImportError:
            print("âš ï¸ PyTorchæœªå®‰è£…")
            return None
    
    @staticmethod
    def check_disk_usage():
        """æ£€æŸ¥ç£ç›˜ä½¿ç”¨æƒ…å†µ"""
        result = subprocess.run(['df', '-h', '/content'], 
                              capture_output=True, text=True)
        print("ç£ç›˜ä½¿ç”¨æƒ…å†µ:")
        print(result.stdout)
    
    @staticmethod
    def nvidia_smi():
        """è¿è¡Œnvidia-smiå‘½ä»¤"""
        result = subprocess.run(['nvidia-smi'], 
                              capture_output=True, text=True)
        if result.returncode == 0:
            print("NVIDIA GPUä¿¡æ¯:")
            print(result.stdout)
        else:
            print("âš ï¸ nvidia-smiä¸å¯ç”¨")

def quick_setup(github_repo: Optional[str] = None):
    """å¿«é€Ÿè®¾ç½®Colabç¯å¢ƒ"""
    print("ğŸš€ å¿«é€Ÿè®¾ç½®Colabç¯å¢ƒ...")
    
    sync = ColabSync()
    
    # è®¾ç½®ç¯å¢ƒ
    sync.setup_colab_environment()
    
    # åŒæ­¥ä»£ç 
    if github_repo:
        sync.sync_from_github(github_repo)
    else:
        print("ğŸ’¡ æç¤º: è¯·æ‰‹åŠ¨ä¸Šä¼ ä»£ç æˆ–æä¾›GitHubä»“åº“URL")
    
    # å®‰è£…ä¾èµ–
    sync.install_requirements()
    
    # è®¾ç½®Pythonè·¯å¾„
    sync.setup_python_path()
    
    print("âœ… ç¯å¢ƒè®¾ç½®å®Œæˆï¼")
    
    # æ˜¾ç¤ºç³»ç»Ÿä¿¡æ¯
    monitor = ColabMonitor()
    monitor.check_gpu_memory()
    monitor.check_disk_usage()
    
    return sync

# ä¾¿æ·å‡½æ•°
def setup_wandb(project_name: str = 'ai-ml-pipeline'):
    """è®¾ç½®Weights & Biases"""
    try:
        import wandb
        wandb.login()
        wandb.init(project=project_name)
        print("âœ… Wandbå·²åˆå§‹åŒ–")
        return True
    except Exception as e:
        print(f"âš ï¸ Wandbè®¾ç½®å¤±è´¥: {e}")
        return False

def download_from_url(url: str, filename: str):
    """ä»URLä¸‹è½½æ–‡ä»¶"""
    print(f"ğŸ“¥ ä¸‹è½½æ–‡ä»¶: {filename}")
    
    response = requests.get(url, stream=True)
    if response.status_code == 200:
        with open(filename, 'wb') as f:
            for chunk in response.iter_content(chunk_size=8192):
                f.write(chunk)
        print(f"âœ… ä¸‹è½½å®Œæˆ: {filename}")
    else:
        print(f"âš ï¸ ä¸‹è½½å¤±è´¥: {response.status_code}")

def create_colab_notebook(template_name: str = 'basic_training'):
    """åˆ›å»ºColabç¬”è®°æœ¬æ¨¡æ¿"""
    templates = {
        'basic_training': {
            'cells': [
                {
                    'cell_type': 'markdown',
                    'source': ['# AI ML Pipeline - åŸºç¡€è®­ç»ƒæ¨¡æ¿\n']
                },
                {
                    'cell_type': 'code',
                    'source': [
                        '# å¿«é€Ÿè®¾ç½®ç¯å¢ƒ\n',
                        'from utils.colab_utils import quick_setup\n',
                        'sync = quick_setup("your-github-repo")\n'
                    ]
                },
                {
                    'cell_type': 'code',
                    'source': [
                        '# å¼€å§‹è®­ç»ƒ\n',
                        'from src.training.train import main\n',
                        'main()\n'
                    ]
                }
            ]
        }
    }
    
    if template_name in templates:
        notebook_path = f'colab_templates/{template_name}.ipynb'
        with open(notebook_path, 'w', encoding='utf-8') as f:
            json.dump({
                'cells': templates[template_name]['cells'],
                'metadata': {
                    'colab': {'provenance': []},
                    'kernelspec': {'display_name': 'Python 3', 'name': 'python3'},
                    'language_info': {'name': 'python'}
                },
                'nbformat': 4,
                'nbformat_minor': 0
            }, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… ç¬”è®°æœ¬æ¨¡æ¿å·²åˆ›å»º: {notebook_path}")
    else:
        print(f"âš ï¸ æœªçŸ¥æ¨¡æ¿: {template_name}") 